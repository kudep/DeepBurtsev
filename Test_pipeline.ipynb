{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script.core.transformers import *\n",
    "from script.core.models import skmodel, sktransformer, BaseModel\n",
    "from script.core.dataset import Dataset\n",
    "from script.core.utils import read_dataset, get_result\n",
    "from script.core.pipeline import PrepPipeline, Pipeline\n",
    "\n",
    "# linear models\n",
    "from sklearn.svm import LinearSVC\n",
    "# sklearn feachure extractors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n",
    "\n",
    "from script.core.utils import logging\n",
    "import datetime\n",
    "from os.path import join, isfile\n",
    "\n",
    "import secrets\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read csv file and create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Watcher(Dataset):\n",
    "    def __init__(self, data, date, language, dataset_name, seed=None, classes_description=None, root=None,\n",
    "                 *args, **kwargs):\n",
    "\n",
    "        super().__init__(data, seed, classes_description, *args, **kwargs)\n",
    "\n",
    "        self.date = '{}-{}-{}'.format(date.year, date.month, date.day)\n",
    "\n",
    "        if root is None:\n",
    "            root = '/home/mks/projects/intent_classification_script/'\n",
    "\n",
    "        self.conf_dict = join(root, 'data', language, dataset_name, 'log_data')\n",
    "        self.save_path = join(self.conf_dict, 'data')\n",
    "\n",
    "    def test_config(self, conf):\n",
    "        self.add_config(conf)\n",
    "        status = self.check_config(self.pipeline_config)\n",
    "\n",
    "        if isinstance(status, bool):\n",
    "            if status:\n",
    "                # self.save_data(self.pipeline_config)\n",
    "                return False\n",
    "        elif isinstance(status, str):\n",
    "            self.load_data(status)\n",
    "            return True\n",
    "        else:\n",
    "            print(type(status))\n",
    "            raise ValueError('Incorrect')\n",
    "\n",
    "        return self\n",
    "\n",
    "    def check_config(self, conf):\n",
    "        with open(join(self.conf_dict, 'pipe_conf_dict.json'), 'r+') as d:\n",
    "            conf_ = json.load(d)\n",
    "\n",
    "            if len(list(conf_.keys())) == 0:\n",
    "                d.close()\n",
    "                return True\n",
    "            else:\n",
    "                coincidence = False\n",
    "                for name in conf_.keys():\n",
    "                    if conf_[name] == conf:\n",
    "                        coincidence = True\n",
    "                        d.close()\n",
    "                        return name\n",
    "                if not coincidence:\n",
    "                    d.close()\n",
    "                    return True\n",
    "        return None\n",
    "\n",
    "    def save_data(self):\n",
    "        names = self.data.keys()\n",
    "        dataframes = []\n",
    "        datanames = []\n",
    "        for name in names:\n",
    "            if isinstance(self.data[name], pd.DataFrame):\n",
    "                dataframes.append(self.data[name])\n",
    "                datanames.append(name)\n",
    "        data = pd.concat(dataframes, keys=datanames)\n",
    "\n",
    "        # saving in file\n",
    "        secret_name = secrets.token_hex(nbytes=16)\n",
    "\n",
    "        if not os.path.isdir(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "\n",
    "        path = join(self.save_path, secret_name)  # + '.csv'\n",
    "        data.to_csv(path)\n",
    "\n",
    "        # write in conf_dict.json\n",
    "        if isfile(join(self.conf_dict, 'pipe_conf_dict.json')):\n",
    "            with open(join(self.conf_dict, 'pipe_conf_dict.json'), 'r') as d:\n",
    "                conf_ = json.load(d)\n",
    "                d.close()\n",
    "\n",
    "            conf_[secret_name] = self.pipeline_config\n",
    "            with open(join(self.conf_dict, 'pipe_conf_dict.json'), 'w') as d:\n",
    "                line = json.dumps(conf_)\n",
    "                d.write(line)\n",
    "                d.close()\n",
    "\n",
    "        else:\n",
    "            conf_ = dict()\n",
    "            conf_[secret_name] = self.pipeline_config\n",
    "            with open(join(self.conf_dict, 'pipe_conf_dict.json'), 'w') as d:\n",
    "                line = json.dumps(conf_)\n",
    "                d.write(line)\n",
    "                d.close()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def load_data(self, name):\n",
    "        filepath = join(self.save_path, name)\n",
    "        file = open(filepath, 'r')\n",
    "        data = pd.read_csv(file)\n",
    "        file.close()\n",
    "\n",
    "        request, report = self.main_names\n",
    "\n",
    "        keys = list(data['Unnamed: 0'].unique())\n",
    "        data_keys = list(self.data.keys())\n",
    "\n",
    "        for key in keys:\n",
    "            if key not in data_keys:\n",
    "                self.data[key] = {}\n",
    "            self.data[key][request] = data[data['Unnamed: 0'] == key][request]\n",
    "            self.data[key][report] = data[data['Unnamed: 0'] == key][report]\n",
    "\n",
    "        for key in data_keys:\n",
    "            if key not in keys:\n",
    "                self.del_data([key])\n",
    "\n",
    "        del data\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2910: DtypeWarning: Columns (6,7,10,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "def init_dataset_tiny(file_path, language, dataset_name, date, seed=42):\n",
    "    pure_data = read_dataset(file_path, True, True)  # It not default meanings!!!\n",
    "    start_dataset = Watcher(pure_data, date, language, dataset_name,\n",
    "                            seed=seed)  # classes_descriptions = {} we can do it\n",
    "\n",
    "    ######################################################################################\n",
    "    dataset = start_dataset.split([0.1, 0.1])\n",
    "    data = dataset.data['test']\n",
    "    start_dataset = Watcher(data, date, language, dataset_name, seed)\n",
    "    ######################################################################################\n",
    "\n",
    "    return start_dataset\n",
    "\n",
    "date = datetime.datetime.now()\n",
    "dataset_name = 'vkusvill'\n",
    "language = 'russian'\n",
    "file_path = join('./data', language, dataset_name, 'data', 'vkusvill_all_categories.csv')\n",
    "\n",
    "dataset = init_dataset_tiny(file_path, language, dataset_name, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Watcher at 0x7fa043003f28>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.load_data('d90f5f6b59dbff9deec6116641b5973e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'valid', 'test', 'train_vec', 'valid_vec', 'test_vec'])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'request': <3667x23139 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 107315 stored elements in Compressed Sparse Row format>, 'report': 0       15\n",
      "1        3\n",
      "2       10\n",
      "3        7\n",
      "4       13\n",
      "5        2\n",
      "6        3\n",
      "7       12\n",
      "8        7\n",
      "9        1\n",
      "10       6\n",
      "11       6\n",
      "12      10\n",
      "13       4\n",
      "14       7\n",
      "15       6\n",
      "16       3\n",
      "17       2\n",
      "18       6\n",
      "19       6\n",
      "20       6\n",
      "21       3\n",
      "22       7\n",
      "23      15\n",
      "24       3\n",
      "25       1\n",
      "26      13\n",
      "27       1\n",
      "28       6\n",
      "29       1\n",
      "        ..\n",
      "3637     3\n",
      "3638    11\n",
      "3639     6\n",
      "3640    11\n",
      "3641    13\n",
      "3642     3\n",
      "3643    10\n",
      "3644    10\n",
      "3645     3\n",
      "3646     7\n",
      "3647     6\n",
      "3648     6\n",
      "3649    11\n",
      "3650     1\n",
      "3651     7\n",
      "3652     1\n",
      "3653    12\n",
      "3654    16\n",
      "3655    13\n",
      "3656     2\n",
      "3657     3\n",
      "3658    15\n",
      "3659     2\n",
      "3660    12\n",
      "3661     3\n",
      "3662     7\n",
      "3663     1\n",
      "3664     6\n",
      "3665     6\n",
      "3666    12\n",
      "Name: report, Length: 3667, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.data['train_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "dataset.split()\n",
    "dataset = tfidf_.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Watcher at 0x7fa06d74b2e8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid', 'test', 'train_vec', 'valid_vec', 'test_vec'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_conf = {'op_type': 'transformer',\n",
    "            'name': 'Speller',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base'],\n",
    "            'path': './DeepPavlov/deeppavlov/configs/error_model/brillmoore_kartaslov_ru.json'}\n",
    "\n",
    "tok_conf = {'op_type': 'transformer',\n",
    "            'name': 'Tokenizer',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base']}\n",
    "\n",
    "lem_conf = {'op_type': 'transformer',\n",
    "            'name': 'Lemmatizer',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base']}\n",
    "\n",
    "concat = TextConcat()\n",
    "\n",
    "tfidf_conf_1 = {'op_type': 'vectorizer', 'name': 'tf-idf vectorizer',\n",
    "                'request_names': ['train', 'valid', 'test'], 'new_names': ['train_vec', 'valid_vec', 'test_vec']}\n",
    "tfidf_conf_2 = {'op_type': 'vectorizer', 'name': 'tf-idf_vectorizer',\n",
    "                'request_names': ['train', 'valid', 'test'], 'new_names': ['train_vec', 'valid_vec', 'test_vec']}\n",
    "tfidf_ = sktransformer(tfidf, tfidf_conf_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_0 = {'op_type': 'model', 'name': 'Linear SVC',\n",
    "          'fit_names': ['train_vec'], 'new_names': ['predicted_test'],\n",
    "          'predict_names': ['test_vec']}\n",
    "\n",
    "LinearSVC = skmodel(LinearSVC, conf_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_1 = [(Speller, ), (Tokenizer, ), (Lemmatizer,), (concat, ), (tfidf_, )]\n",
    "pipeline_1 = PrepPipeline(pipe_1)\n",
    "\n",
    "pipe_2 = [(Speller, ), (Tokenizer, ), (Lemmatizer,), (concat, ), (tfidf_, )]\n",
    "pipeline_2 = PrepPipeline(pipe_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
