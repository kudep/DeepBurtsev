{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script.core.transformers import *\n",
    "from script.core.models import skmodel, sktransformer, BaseModel\n",
    "from script.core.dataset import Dataset\n",
    "from script.core.utils import read_dataset, get_result\n",
    "from script.core.pipeline import BasePipeline\n",
    "\n",
    "# linear models\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# sklearn feachure extractors\n",
    "from sklearn.feature_extraction.text import CountVectorizer as count\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from intent_classifier.intent_model.model_wrap import KerasMulticlassModel as DilyaraModel\n",
    "from script.models.cnn import CNN\n",
    "from script.models.dcnn import DCNN\n",
    "\n",
    "from script.core.utils import logging\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read csv file and create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/russian/data/vkusvill_all_categories.csv'\n",
    "global_data = read_dataset(path)\n",
    "dataset = Dataset(global_data, seed=42)\n",
    "dataset = dataset.split([0.1, 0.1])\n",
    "\n",
    "print(dataset.data.keys())\n",
    "print(len(dataset.data['valid']))\n",
    "\n",
    "data = dataset.data['test']\n",
    "dataset = Dataset(data, seed=42)\n",
    "print(dataset.data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_conf = {'op_type': 'transformer',\n",
    "            'name': 'Speller',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base'],\n",
    "            'path': './DeepPavlov/deeppavlov/configs/error_model/brillmoore_kartaslov_ru.json'}\n",
    "\n",
    "tok_conf = {'op_type': 'transformer',\n",
    "            'name': 'Tokenizer',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base']}\n",
    "\n",
    "lem_conf = {'op_type': 'transformer',\n",
    "            'name': 'Lemmatizer',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base']}\n",
    "\n",
    "concat = TextConcat()\n",
    "\n",
    "tfidf_conf_1 = {'op_type': 'vectorizer', 'name': 'tf-idf vectorizer',\n",
    "                'request_names': ['train', 'valid', 'test'], 'new_names': ['train_vec', 'valid_vec', 'test_vec']}\n",
    "tfidf_conf_2 = {'op_type': 'vectorizer', 'name': 'tf-idf_vectorizer',\n",
    "                'request_names': ['train', 'valid', 'test'], 'new_names': ['train_vec', 'valid_vec', 'test_vec']}\n",
    "tfidf_ = sktransformer(tfidf, tfidf_conf_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetCNN(BaseModel):\n",
    "    def init_model(self, dataset):\n",
    "        classes = dataset.get_classes()\n",
    "        classes = ' '.join([str(x) for x in classes])\n",
    "        self.config['classes'] = classes\n",
    "        \n",
    "        super().init_model(dataset)\n",
    "        \n",
    "        return self\n",
    "\n",
    "def get_cnn_conf(path, emb_path, fit_names=None, predict_names=None, new_names=None):\n",
    "    with open(path, 'r') as conf:\n",
    "        config = json.load(conf)\n",
    "\n",
    "    config['op_type'] = 'model'\n",
    "    config['name'] = 'cnn'\n",
    "    \n",
    "    if fit_names is not None:\n",
    "        config['fit_names'] = fit_names\n",
    "    else:\n",
    "        config['fit_names'] = ['train_vec']\n",
    "    \n",
    "    if predict_names is not None:\n",
    "        config['predict_names'] = predict_names\n",
    "    else:\n",
    "        config['predict_names'] = ['test_vec']\n",
    "        \n",
    "    if new_names is not None:\n",
    "        config['new_names'] = new_names\n",
    "    else:\n",
    "        config['new_names'] = ['predicted_test']\n",
    "    \n",
    "    config['fasttext_model'] = emb_path\n",
    "    \n",
    "    return config\n",
    "\n",
    "path_0 = './configs/models/CNN/CNN_opt.json'\n",
    "path_1 = './configs/models/CNN/cnn.json'\n",
    "emb_path = './data/russian/embeddings/ft_0.8.3_nltk_yalen_sg_300.bin'\n",
    "\n",
    "# config = get_cnn_conf(path_0, emb_path, fit_names=['train'], predict_names=['test'])\n",
    "# model = GetCNN(DilyaraModel, config)\n",
    "\n",
    "config = get_cnn_conf(path_1, emb_path)\n",
    "model = GetCNN(CNN, config)\n",
    "\n",
    "# config = get_cnn_conf(path_1, emb_path)\n",
    "# model = GetCNN(DCNN, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline with neural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_conf = {'op_type': 'transformer',\n",
    "            'name': 'Tokenizer',\n",
    "            'request_names': ['base'],  # 'train', 'valid', 'test'\n",
    "            'new_names': ['base']}\n",
    "lem_conf = {'op_type': 'transformer',\n",
    "            'name': 'Lemmatizator',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base']}\n",
    "con_conf = {'op_type': 'transformer',\n",
    "            'name': 'Concatenizer',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base']}\n",
    "\n",
    "# (Speller, spl_conf),  (Lemmatizer, lem_conf), (concat, con_conf), \n",
    "\n",
    "\n",
    "neuro_pipe_0 = [(Tokenizer, tok_conf), (FasttextVectorizer,), (model,)]\n",
    "pipeline_8 = BasePipeline(neuro_pipe_0, mode='train', output=None)\n",
    "pipeline_9 = BasePipeline(neuro_pipe_0, mode='infer', output='dataset')\n",
    "\n",
    "data_ = pipeline_9.run(dataset)\n",
    "# data_.data.keys()\n",
    "# data_.data['predicted_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_0 = {'op_type': 'model', 'name': 'Linear Regression',\n",
    "          'fit_names': ['train_vec'], 'new_names': ['predicted_test'],\n",
    "          'predict_names': ['test_vec']}\n",
    "LogisticRegression = skmodel(LogisticRegression, conf_0)\n",
    "LGBMClassifier = skmodel(LGBMClassifier, conf_0)\n",
    "LinearSVC = skmodel(LinearSVC, conf_0)\n",
    "RandomForestClassifier = skmodel(RandomForestClassifier, conf_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline with linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Tokenizer, tok_conf), (Lemmatizer,), (concat, None), \n",
    "pipe_0 = [(tfidf_, tfidf_conf_2), (LogisticRegression,), (GetResultLinear,)]\n",
    "pipeline_0 = BasePipeline(pipe_0, mode='train', output=None)\n",
    "pipeline_1 = BasePipeline(pipe_0, mode='infer', output='dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pipeline_1.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pipeline_1.pipeline_config\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.data['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = dict(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.now()\n",
    "name = '{}-{}-{}.txt'.format(date.year, date.month, date.day)\n",
    "logging(res.data['results'], conf, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline with SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/russian/data/vkusvill_all_categories.csv'\n",
    "global_data = read_dataset(path)\n",
    "dataset = Dataset(global_data, seed=42)\n",
    "dataset = dataset.split(splitting_proportions=[0.99, 0.01])\n",
    "\n",
    "print(dataset.data.keys())\n",
    "print(len(dataset.data['test']))\n",
    "\n",
    "data = dataset.data['test']\n",
    "dataset = Dataset(data, seed=42)\n",
    "print(dataset.data.keys())\n",
    "\n",
    "pipe_1 = [(Speller, spl_conf), (Tokenizer, tok_conf), (Lemmatizer,), (concat, None), (tfidf_, tfidf_conf_2),\n",
    "          (LinearSVC,)]\n",
    "pipeline_2 = BasePipeline(pipe_1, mode='train', output=None)\n",
    "pipeline_3 = BasePipeline(pipe_1, mode='infer', output='dataset')\n",
    "\n",
    "data_ = pipeline_3.run(dataset)\n",
    "data_.data.keys()\n",
    "data_.data['test_new']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Run pipeline with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/russian/data/vkusvill_all_categories.csv'\n",
    "global_data = read_dataset(path)\n",
    "dataset = Dataset(global_data, seed=42)\n",
    "dataset = dataset.split(splitting_proportions=[0.99, 0.01])\n",
    "\n",
    "print(dataset.data.keys())\n",
    "print(len(dataset.data['test']))\n",
    "\n",
    "data = dataset.data['test']\n",
    "dataset = Dataset(data, seed=42)\n",
    "print(dataset.data.keys())\n",
    "\n",
    "pipe_2 = [(Speller, spl_conf), (Tokenizer, tok_conf), (Lemmatizer,), (concat, None), (tfidf_, tfidf_conf_2),\n",
    "          (RandomForestClassifier,)]\n",
    "pipeline_4 = BasePipeline(pipe_2, mode='train', output=None)\n",
    "pipeline_5 = BasePipeline(pipe_2, mode='infer', output='dataset')\n",
    "\n",
    "data_ = pipeline_5.run(dataset)\n",
    "data_.data.keys()\n",
    "data_.data['test_new']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline with GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/russian/data/vkusvill_all_categories.csv'\n",
    "global_data = read_dataset(path)\n",
    "dataset = Dataset(global_data, seed=42)\n",
    "dataset = dataset.split(splitting_proportions=[0.99, 0.01])\n",
    "\n",
    "print(dataset.data.keys())\n",
    "print(len(dataset.data['test']))\n",
    "\n",
    "data = dataset.data['test']\n",
    "dataset = Dataset(data, seed=42)\n",
    "print(dataset.data.keys())\n",
    "\n",
    "pipe_3 = [(Speller, spl_conf), (Tokenizer, tok_conf), (Lemmatizer,), (concat, None), (tfidf_, tfidf_conf_2),\n",
    "          (LGBMClassifier,)]\n",
    "pipeline_6 = BasePipeline(pipe_3, mode='train', output=None)\n",
    "pipeline_7 = BasePipeline(pipe_3, mode='infer', output='dataset')\n",
    "\n",
    "data_ = pipeline_7.run(dataset)\n",
    "data_.data.keys()\n",
    "data_.data['test_new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
