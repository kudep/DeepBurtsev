{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-03-25 08:20:19.960 DEBUG in 'matplotlib.backends'['__init__'] at line 90: backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "from script.core.transformers import *\n",
    "from script.core.models import *\n",
    "from script.core.dataset import Watcher\n",
    "from script.core.utils import read_dataset, get_result, logging\n",
    "from script.core.pipeline import Pipeline\n",
    "\n",
    "# linear models\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# sklearn feachure extractors\n",
    "from sklearn.feature_extraction.text import CountVectorizer as count\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Pure DilyaraModel\n",
    "from intent_classifier.intent_model.model_wrap import KerasMulticlassModel as DilyaraModel\n",
    "from script.models.cnn import CNN\n",
    "from script.models.dcnn import DCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read csv file and create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dataset():\n",
    "    path = './data/russian/data/vkusvill_all_categories.csv'\n",
    "    global_data = read_dataset(path)\n",
    "    date = datetime.datetime.now()\n",
    "    dataset = Watcher(global_data, date, seed=42)\n",
    "#     dataset = dataset.split([0.1, 0.1])\n",
    "\n",
    "#     print(dataset.data.keys())\n",
    "#     print(len(dataset.data['valid']))\n",
    "\n",
    "#     data = dataset.data['test']\n",
    "#     dataset = Watcher(data, date, seed=42)\n",
    "#     print(dataset.data.keys())\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_conf = {'op_type': 'transformer',\n",
    "            'name': 'Speller',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base'],\n",
    "            'path': './DeepPavlov/deeppavlov/configs/error_model/brillmoore_kartaslov_ru.json'}\n",
    "\n",
    "tok_conf = {'op_type': 'transformer',\n",
    "            'name': 'Tokenizer',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base']}\n",
    "\n",
    "lem_conf = {'op_type': 'transformer',\n",
    "            'name': 'Lemmatizer',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base']}\n",
    "\n",
    "concat = TextConcat()\n",
    "\n",
    "tfidf_conf_1 = {'op_type': 'vectorizer', 'name': 'tf-idf vectorizer',\n",
    "                'request_names': ['train', 'valid', 'test'], 'new_names': ['train_vec', 'valid_vec', 'test_vec']}\n",
    "tfidf_conf_2 = {'op_type': 'vectorizer', 'name': 'tf-idf_vectorizer',\n",
    "                'request_names': ['train', 'valid', 'test'], 'new_names': ['train_vec', 'valid_vec', 'test_vec']}\n",
    "tfidf_ = sktransformer(tfidf, tfidf_conf_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline with Neural Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetCNN(BaseModel):\n",
    "    def init_model(self, dataset):\n",
    "        classes = dataset.get_classes()\n",
    "        classes = ' '.join([str(x) for x in classes])\n",
    "        self.config['classes'] = classes\n",
    "        \n",
    "        super().init_model(dataset)\n",
    "        \n",
    "        return self\n",
    "\n",
    "def get_cnn_conf(path, emb_path, fit_names=None, predict_names=None, new_names=None):\n",
    "    with open(path, 'r') as conf:\n",
    "        config = json.load(conf)\n",
    "\n",
    "    config['op_type'] = 'model'\n",
    "    config['name'] = 'cnn'\n",
    "    \n",
    "    if fit_names is not None:\n",
    "        config['fit_names'] = fit_names\n",
    "    else:\n",
    "        config['fit_names'] = ['train_vec']\n",
    "    \n",
    "    if predict_names is not None:\n",
    "        config['predict_names'] = predict_names\n",
    "    else:\n",
    "        config['predict_names'] = ['test_vec']\n",
    "        \n",
    "    if new_names is not None:\n",
    "        config['new_names'] = new_names\n",
    "    else:\n",
    "        config['new_names'] = ['predicted_test']\n",
    "    \n",
    "    config['fasttext_model'] = emb_path\n",
    "    \n",
    "    return config\n",
    "\n",
    "path_0 = './configs/models/CNN/CNN_opt.json'\n",
    "path_1 = './configs/models/CNN/cnn.json'\n",
    "emb_path = './data/russian/embeddings/ft_0.8.3_nltk_yalen_sg_300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = get_cnn_conf(path_0, emb_path, fit_names=['train'], predict_names=['test'])\n",
    "# dataset = init_dataset()\n",
    "# Dilyara = GetCNN(DilyaraModel, config)\n",
    "# neuro_pipe = [(Tokenizer, ), (Lemmatizer,), (TextConcat,), (Dilyara,), (GetResult,)]\n",
    "# pipeline_0 = Pipeline(neuro_pipe, mode='infer', output='dataset')\n",
    "# neurodata = pipeline_0.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2910: DtypeWarning: Columns (6,7,10,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      " 12%|█▏        | 524/4475 [00:00<00:00, 5236.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'valid', 'test'])\n",
      "4475\n",
      "dict_keys(['base'])\n",
      "[ Starting tokenization ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:00<00:00, 5174.37it/s]\n",
      "  0%|          | 0/3599 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Tokenization was done. ]\n",
      "[ Starting vectorization ... ]\n",
      "[ Vectorization of train part of dataset ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3599/3599 [00:01<00:00, 1820.66it/s]\n",
      " 38%|███▊      | 168/438 [00:00<00:00, 1677.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Vectorization of valid part of dataset ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:00<00:00, 1744.82it/s]\n",
      " 35%|███▍      | 152/438 [00:00<00:00, 1493.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Vectorization of test part of dataset ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:00<00:00, 1555.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Vectorization was ended. ]\n",
      "[ Initializing intent_model from scratch ]\n",
      "\n",
      "____Training over 3599 samples____\n",
      "\n",
      "\n",
      "train -->\tupdates: 1\tloss: 0.36509931087493896\tfmeasure: 0.0\t \n",
      "epochs_done: 1\n",
      "train -->\tupdates: 58\tloss: 0.2574595808982849\tfmeasure: 0.4337348937988281\t \n",
      "epochs_done: 2\n"
     ]
    }
   ],
   "source": [
    "config = get_cnn_conf(path_1, emb_path)\n",
    "dataset = init_dataset()\n",
    "WCNN = GetCNN(CNN, config)\n",
    "neuro_pipe = [(Tokenizer, ), (FasttextVectorizer,), (WCNN,)]\n",
    "pipeline_1 = Pipeline(neuro_pipe, mode='infer', output='dataset')\n",
    "neurodata = pipeline_1.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid', 'test', 'train_vec', 'valid_vec', 'test_vec', 'predicted_test'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurodata.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(439, 17)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = neurodata.data['test_vec']['report']\n",
    "a_ = np.zeros((len(a), len(a[0])))\n",
    "for i, x in enumerate(a):\n",
    "    for j, y in enumerate(x):\n",
    "        a_[i][j] = y\n",
    "a_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 12,  2,  0,  5, 14, 11,  2,  5,  2,  5,  2,  2, 15,  5,  1,  5,\n",
       "        5,  1,  0,  5,  1,  1,  2,  5,  1,  5,  5,  2,  9,  2,  0,  2,  9,\n",
       "        1,  9,  7,  0,  5,  1, 10,  6, 10,  6,  1,  0,  9,  9, 12,  3,  2,\n",
       "       11,  5,  1,  5,  8,  0, 12,  5,  2,  9,  5, 15,  1, 12,  8,  5,  0,\n",
       "        5,  2,  2,  5,  1,  9,  2,  2,  5,  9, 11,  0,  5,  9,  0, 14,  0,\n",
       "        6,  5, 12,  0,  5, 12,  1,  5,  9,  2,  0, 12,  2,  2,  5, 14,  5,\n",
       "       12,  5, 10,  6,  6,  5,  6,  2, 12,  7,  6,  9,  5,  6,  1,  6,  5,\n",
       "        9,  1, 14,  1,  0,  9,  2,  0,  2,  1,  1,  1, 15,  5,  9,  5, 12,\n",
       "        9,  2,  1,  0,  2, 11,  6,  0,  2,  2, 11, 10,  9,  6,  5,  1,  2,\n",
       "        5,  2,  5,  1,  2,  5,  5,  5,  3,  7,  1,  5,  0,  3, 10, 12,  6,\n",
       "        5,  3, 15,  5,  5,  6,  5,  2, 10,  1,  2, 12,  0,  5,  5,  5, 12,\n",
       "       13,  6,  2,  5, 12,  5,  0,  2, 12, 10,  6,  2,  5,  6,  1,  5,  0,\n",
       "       12,  5,  2, 11,  8,  5,  5, 10,  5,  5, 14,  3, 10,  5,  9,  1,  3,\n",
       "        5,  1,  6, 10, 12,  5,  5, 12,  5,  1,  2, 15,  1,  2,  5, 10, 11,\n",
       "        5,  9, 12,  1,  2,  2, 11,  1,  5, 12, 14,  9,  5, 11, 14,  5,  5,\n",
       "        9,  1,  5,  5,  2,  6,  1,  1,  6,  5,  6,  5,  6,  0,  1,  6,  0,\n",
       "        2,  2,  5,  5,  5,  5, 11,  5,  5, 13,  1,  5,  6,  2,  0,  3,  1,\n",
       "        2, 10, 10,  9,  2,  2, 11, 11, 15,  1,  1,  2,  5,  5,  1,  3,  9,\n",
       "        6,  9,  2, 10,  3,  2,  1, 10, 12,  9,  5,  9,  8, 12,  5,  2,  2,\n",
       "        6, 11,  5,  1, 12,  5,  9, 11,  6,  5,  8, 12, 14,  0, 11,  5, 14,\n",
       "        8,  0,  1, 12,  9, 11,  6,  6,  3,  0,  3,  5, 12,  0,  9,  2,  2,\n",
       "        6,  1, 12,  8,  9, 10,  2, 15,  8,  2,  2, 11, 12,  2,  2, 12,  1,\n",
       "        2,  0, 12,  9,  5,  8,  5,  1, 10,  9,  6,  1,  5,  0,  2,  0, 12,\n",
       "       14, 11,  2,  5,  2,  6,  5, 11,  9,  7, 10,  5,  1,  0,  6,  2,  2,\n",
       "        6,  9,  1, 11,  2,  2, 12,  5, 10,  6,  6, 10,  1,  5,  5, 12, 12,\n",
       "        9, 15, 11, 14,  1,  2, 11,  6,  3,  1,  0,  1,  5,  3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.argmax(a_, axis=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 13,  3,  1,  6, 15, 12,  3,  6,  3,  6,  3,  3, 16,  6,  2,  6,\n",
       "        6,  2,  1,  6,  2,  2,  3,  6,  2,  6,  6,  3, 10,  3,  1,  3, 10,\n",
       "        2, 10,  8,  1,  6,  2, 11,  7, 11,  7,  2,  1, 10, 10, 13,  4,  3,\n",
       "       12,  6,  2,  6,  9,  1, 13,  6,  3, 10,  6, 16,  2, 13,  9,  6,  1,\n",
       "        6,  3,  3,  6,  2, 10,  3,  3,  6, 10, 12,  1,  6, 10,  1, 15,  1,\n",
       "        7,  6, 13,  1,  6, 13,  2,  6, 10,  3,  1, 13,  3,  3,  6, 15,  6,\n",
       "       13,  6, 11,  7,  7,  6,  7,  3, 13,  8,  7, 10,  6,  7,  2,  7,  6,\n",
       "       10,  2, 15,  2,  1, 10,  3,  1,  3,  2,  2,  2, 16,  6, 10,  6, 13,\n",
       "       10,  3,  2,  1,  3, 12,  7,  1,  3,  3, 12, 11, 10,  7,  6,  2,  3,\n",
       "        6,  3,  6,  2,  3,  6,  6,  6,  4,  8,  2,  6,  1,  4, 11, 13,  7,\n",
       "        6,  4, 16,  6,  6,  7,  6,  3, 11,  2,  3, 13,  1,  6,  6,  6, 13,\n",
       "       14,  7,  3,  6, 13,  6,  1,  3, 13, 11,  7,  3,  6,  7,  2,  6,  1,\n",
       "       13,  6,  3, 12,  9,  6,  6, 11,  6,  6, 15,  4, 11,  6, 10,  2,  4,\n",
       "        6,  2,  7, 11, 13,  6,  6, 13,  6,  2,  3, 16,  2,  3,  6, 11, 12,\n",
       "        6, 10, 13,  2,  3,  3, 12,  2,  6, 13, 15, 10,  6, 12, 15,  6,  6,\n",
       "       10,  2,  6,  6,  3,  7,  2,  2,  7,  6,  7,  6,  7,  1,  2,  7,  1,\n",
       "        3,  3,  6,  6,  6,  6, 12,  6,  6, 14,  2,  6,  7,  3,  1,  4,  2,\n",
       "        3, 11, 11, 10,  3,  3, 12, 12, 16,  2,  2,  3,  6,  6,  2,  4, 10,\n",
       "        7, 10,  3, 11,  4,  3,  2, 11, 13, 10,  6, 10,  9, 13,  6,  3,  3,\n",
       "        7, 12,  6,  2, 13,  6, 10, 12,  7,  6,  9, 13, 15,  1, 12,  6, 15,\n",
       "        9,  1,  2, 13, 10, 12,  7,  7,  4,  1,  4,  6, 13,  1, 10,  3,  3,\n",
       "        7,  2, 13,  9, 10, 11,  3, 16,  9,  3,  3, 12, 13,  3,  3, 13,  2,\n",
       "        3,  1, 13, 10,  6,  9,  6,  2, 11, 10,  7,  2,  6,  1,  3,  1, 13,\n",
       "       15, 12,  3,  6,  3,  7,  6, 12, 10,  8, 11,  6,  2,  1,  7,  3,  3,\n",
       "        7, 10,  2, 12,  3,  3, 13,  6, 11,  7,  7, 11,  2,  6,  6, 13, 13,\n",
       "       10, 16, 12, 15,  2,  3, 12,  7,  4,  2,  1,  2,  6,  4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array(neurodata.data['test']['report'])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = neurodata.data['predicted_test']\n",
    "\n",
    "preds = pred_data[0]\n",
    "for x in pred_data[1:]:\n",
    "    preds = np.concatenate((preds, x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(448, 17)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(448,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([12, 12,  0, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12,  1, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12,  6, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        6, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12,  1, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12,  0, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  0, 12,\n",
       "       12,  6, 12, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12,  0, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12,  6, 12, 12, 12, 12, 12, 12, 12,  1, 12, 12, 12, 12, 12, 12,\n",
       "       11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  6, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12,  6, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11,\n",
       "        6, 12, 12, 12, 12,  0, 12, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  1, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.argmax(preds, axis=1)\n",
    "print(preds.shape)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2910: DtypeWarning: Columns (6,7,10,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  2%|▏         | 1032/45171 [00:00<00:08, 5156.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Starting tokenization ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45171/45171 [00:09<00:00, 4960.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Tokenization was done. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 188/36221 [00:00<00:19, 1877.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Starting vectorization ... ]\n",
      "[ Vectorization of train part of dataset ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36221/36221 [00:20<00:00, 1768.85it/s]\n",
      "  4%|▍         | 174/4475 [00:00<00:02, 1733.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Vectorization of valid part of dataset ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:02<00:00, 1714.01it/s]\n",
      "  4%|▎         | 165/4475 [00:00<00:02, 1648.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Vectorization of test part of dataset ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:02<00:00, 1722.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Vectorization was ended. ]\n",
      "[ Initializing intent_model from scratch ]\n",
      "\n",
      "____Training over 36221 samples____\n",
      "\n",
      "\n",
      "train -->\tupdates: 1\tloss: 0.35727494955062866\tfmeasure: 0.0\t \n",
      "train -->\tupdates: 501\tloss: 0.15314698219299316\tfmeasure: 0.5957446098327637\t \n",
      "epochs_done: 1\n",
      "train -->\tupdates: 567\tloss: 0.1640733778476715\tfmeasure: 0.5208332538604736\t \n",
      "train -->\tupdates: 1067\tloss: 0.15857265889644623\tfmeasure: 0.4905659556388855\t \n",
      "epochs_done: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "config = get_cnn_conf(path_1, emb_path)\n",
    "dataset = init_dataset()\n",
    "WCNN = GetCNN(CNN, config)\n",
    "neuro_pipe = [(Tokenizer, ), (FasttextVectorizer,), (WCNN,), (GetResult,)]\n",
    "pipeline_1 = Pipeline(neuro_pipe, mode='infer', output='dataset')\n",
    "neurodata = pipeline_1.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = get_cnn_conf(path_1, emb_path)\n",
    "# dataset = init_dataset()\n",
    "# dCNN = GetCNN(DCNN, config)\n",
    "# neuro_pipe = [(Tokenizer, ), (FasttextVectorizer,), (dCNN,), (GetResult,)]\n",
    "# pipeline_2 = Pipeline(neuro_pipe, mode='infer', output='dataset')\n",
    "# neurodata = pipeline_2.run(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines with Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_0 = {'op_type': 'model', 'name': 'Linear Regression',\n",
    "          'fit_names': ['train_vec'], 'new_names': ['predicted_test'],\n",
    "          'predict_names': ['test_vec']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline with linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2910: DtypeWarning: Columns (6,7,10,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'valid', 'test'])\n",
      "4475\n",
      "dict_keys(['base'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n",
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# (Tokenizer, tok_conf), (Lemmatizer,), (concat, None),\n",
    "dataset = init_dataset()\n",
    "LR = skmodel(LogisticRegression, conf_0)\n",
    "\n",
    "pipe_0 = [(tfidf_, tfidf_conf_2), (LR,), (GetResultLinear_W,)]\n",
    "pipeline_0 = Pipeline(pipe_0, mode='train', output=None)\n",
    "pipeline_1 = Pipeline(pipe_0, mode='infer', output='dataset')\n",
    "res = pipeline_1.run(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline with SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2910: DtypeWarning: Columns (6,7,10,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      " 12%|█▏        | 538/4475 [00:00<00:00, 5374.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'valid', 'test'])\n",
      "4475\n",
      "dict_keys(['base'])\n",
      "[ Starting tokenization ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:00<00:00, 5276.25it/s]\n",
      "2018-03-25 07:17:59.720 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 16: Loading dictionaries from /home/mks/envs/intent_script/lib/python3.6/site-packages/pymorphy2_dicts/data\n",
      "2018-03-25 07:17:59.749 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 20: format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n",
      "  0%|          | 16/4475 [00:00<00:29, 153.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Tokenization was done. ]\n",
      "[ Starting lemmatization ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:26<00:00, 166.86it/s]\n",
      "100%|██████████| 4475/4475 [00:00<00:00, 440164.87it/s]\n",
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Ended lemmatization. ]\n",
      "[ Starting text merging ... ]\n",
      "[ Text concatenation was ended. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "dataset = init_dataset()\n",
    "\n",
    "conf_0['name'] = 'LinearSVC'\n",
    "conf_0['op_type'] = 'model'\n",
    "LSVC = skmodel(LinearSVC, conf_0)\n",
    "\n",
    "# (Speller, spl_conf),\n",
    "\n",
    "pipe_1 = [(Tokenizer, ), (Lemmatizer,), (concat, None), (tfidf_, tfidf_conf_2),\n",
    "          (LSVC,), (GetResultLinear_W,)]\n",
    "pipeline_2 = Pipeline(pipe_1, mode='train', output=None)\n",
    "pipeline_3 = Pipeline(pipe_1, mode='infer', output='dataset')\n",
    "\n",
    "res = pipeline_3.run(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Run pipeline with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2910: DtypeWarning: Columns (6,7,10,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      " 12%|█▏        | 524/4475 [00:00<00:00, 5217.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'valid', 'test'])\n",
      "4475\n",
      "dict_keys(['base'])\n",
      "[ Starting tokenization ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:00<00:00, 5327.62it/s]\n",
      "2018-03-25 07:18:30.321 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 16: Loading dictionaries from /home/mks/envs/intent_script/lib/python3.6/site-packages/pymorphy2_dicts/data\n",
      "2018-03-25 07:18:30.350 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 20: format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n",
      "  0%|          | 0/4475 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Tokenization was done. ]\n",
      "[ Starting lemmatization ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:26<00:00, 168.61it/s]\n",
      "100%|██████████| 4475/4475 [00:00<00:00, 451950.65it/s]\n",
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Ended lemmatization. ]\n",
      "[ Starting text merging ... ]\n",
      "[ Text concatenation was ended. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "dataset = init_dataset()\n",
    "\n",
    "conf_0['name'] = 'RandomForestClassifier'\n",
    "conf_0['op_type'] = 'model'\n",
    "RFC = skmodel(RandomForestClassifier, conf_0)\n",
    "\n",
    "\n",
    "# (Speller, spl_conf), \n",
    "pipe_2 = [(Tokenizer, ), (Lemmatizer,), (concat, None), (tfidf_, tfidf_conf_2),\n",
    "          (RFC,), (GetResultLinear_W,)]\n",
    "pipeline_4 = Pipeline(pipe_2, mode='train', output=None)\n",
    "pipeline_5 = Pipeline(pipe_2, mode='infer', output='dataset')\n",
    "\n",
    "res = pipeline_5.run(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline with GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2910: DtypeWarning: Columns (6,7,10,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      " 11%|█         | 494/4475 [00:00<00:00, 4939.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'valid', 'test'])\n",
      "4475\n",
      "dict_keys(['base'])\n",
      "[ Starting tokenization ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:00<00:00, 5269.91it/s]\n",
      "2018-03-25 07:19:01.60 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 16: Loading dictionaries from /home/mks/envs/intent_script/lib/python3.6/site-packages/pymorphy2_dicts/data\n",
      "2018-03-25 07:19:01.89 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 20: format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n",
      "  0%|          | 0/4475 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Tokenization was done. ]\n",
      "[ Starting lemmatization ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:27<00:00, 164.97it/s]\n",
      "100%|██████████| 4475/4475 [00:00<00:00, 442186.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Ended lemmatization. ]\n",
      "[ Starting text merging ... ]\n",
      "[ Text concatenation was ended. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n",
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "dataset = init_dataset()\n",
    "\n",
    "conf_0['name'] = 'LGBMClassifier'\n",
    "conf_0['op_type'] = 'model'\n",
    "LGBM = skmodel(LGBMClassifier, conf_0)\n",
    "\n",
    "# (Speller, spl_conf),\n",
    "pipe_3 = [(Tokenizer, ), (Lemmatizer,), (concat, None), (tfidf_, tfidf_conf_2),\n",
    "          (LGBM,), (GetResultLinear_W,)]\n",
    "pipeline_6 = Pipeline(pipe_3, mode='train', output=None)\n",
    "pipeline_7 = Pipeline(pipe_3, mode='infer', output='dataset')\n",
    "\n",
    "res = pipeline_7.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
