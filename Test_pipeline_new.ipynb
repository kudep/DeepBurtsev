{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-03 12:41:14.375 DEBUG in 'matplotlib.backends'['__init__'] at line 90: backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from os.path import join\n",
    "\n",
    "from script.core.transformers import *\n",
    "from script.models.cnn import CNN\n",
    "from script.core.models import *\n",
    "from script.core.dataset import Watcher\n",
    "from script.core.utils import read_dataset, get_result, logging\n",
    "from script.core.pipeline import Pipeline\n",
    "\n",
    "# linear models\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# sklearn feachure extractors\n",
    "from sklearn.feature_extraction.text import CountVectorizer as count\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Pure DilyaraModel\n",
    "# from intent_classifier.intent_model.model_wrap import KerasMulticlassModel as DilyaraModel\n",
    "# from script.models.cnn import CNN\n",
    "# from script.models.dcnn import DCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read csv file and create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2910: DtypeWarning: Columns (6,7,10,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "def init_dataset_tiny(file_path, language, dataset_name, date, seed=42):\n",
    "    pure_data = read_dataset(file_path, True, True)  # It not default meanings!!!\n",
    "    start_dataset = Watcher(pure_data, date, language, dataset_name,\n",
    "                            seed=seed)  # classes_descriptions = {} we can do it\n",
    "\n",
    "    ######################################################################################\n",
    "#     dataset = start_dataset.split([0.1, 0.1])\n",
    "#     data = dataset.data['test']\n",
    "#     start_dataset = Watcher(data, date, language, dataset_name, seed)\n",
    "    ######################################################################################\n",
    "\n",
    "    return start_dataset\n",
    "\n",
    "date = datetime.datetime.now()\n",
    "dataset_name = 'vkusvill'\n",
    "language = 'russian'\n",
    "file_path = join('./data', language, dataset_name, 'data', 'vkusvill_all_categories.csv')\n",
    "\n",
    "dataset = init_dataset_tiny(file_path, language, dataset_name, date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_conf = {'op_type': 'transformer',\n",
    "            'name': 'Speller',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base'],\n",
    "            'path': './DeepPavlov/deeppavlov/configs/error_model/brillmoore_kartaslov_ru.json'}\n",
    "\n",
    "tok_conf = {'op_type': 'transformer',\n",
    "            'name': 'Tokenizer',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base']}\n",
    "\n",
    "lem_conf = {'op_type': 'transformer',\n",
    "            'name': 'Lemmatizer',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base']}\n",
    "\n",
    "concat = TextConcat()\n",
    "\n",
    "tfidf_conf_1 = {'op_type': 'vectorizer', 'name': 'tf-idf vectorizer',\n",
    "                'request_names': ['train', 'valid', 'test'], 'new_names': ['train_vec', 'valid_vec', 'test_vec']}\n",
    "tfidf_conf_2 = {'op_type': 'vectorizer', 'name': 'count_vectorizer',\n",
    "                'request_names': ['train', 'valid', 'test'], 'new_names': ['train_vec', 'valid_vec', 'test_vec']}\n",
    "tfidf_ = sktransformer(tfidf, tfidf_conf_1)\n",
    "count_ = sktransformer(count, tfidf_conf_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline with Neural Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetCNN(BaseModel):\n",
    "    def init_model(self, dataset):\n",
    "        classes = dataset.get_classes()\n",
    "        classes = ' '.join([str(x) for x in classes])\n",
    "        self.config['classes'] = classes\n",
    "        \n",
    "        super().init_model(dataset)\n",
    "        \n",
    "        return self\n",
    "\n",
    "def get_cnn_conf(path, emb_path, fit_names=None, predict_names=None, new_names=None):\n",
    "    with open(path, 'r') as conf:\n",
    "        config = json.load(conf)\n",
    "\n",
    "    config['op_type'] = 'model'\n",
    "    config['name'] = 'cnn'\n",
    "    \n",
    "    if fit_names is not None:\n",
    "        config['fit_names'] = fit_names\n",
    "    else:\n",
    "        config['fit_names'] = ['train_vec']\n",
    "    \n",
    "    if predict_names is not None:\n",
    "        config['predict_names'] = predict_names\n",
    "    else:\n",
    "        config['predict_names'] = ['test_vec']\n",
    "        \n",
    "    if new_names is not None:\n",
    "        config['new_names'] = new_names\n",
    "    else:\n",
    "        config['new_names'] = ['predicted_test']\n",
    "    \n",
    "    config['fasttext_model'] = emb_path\n",
    "    \n",
    "    return config\n",
    "\n",
    "path_0 = './configs/models/CNN/CNN_opt.json'\n",
    "path_1 = './configs/models/CNN.json'\n",
    "emb_path = './data/russian/embeddings/ft_0.8.3_nltk_yalen_sg_300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = get_cnn_conf(path_0, emb_path, fit_names=['train'], predict_names=['test'])\n",
    "# dataset = init_dataset()\n",
    "# Dilyara = GetCNN(DilyaraModel, config)\n",
    "# neuro_pipe = [(Tokenizer, ), (Lemmatizer,), (TextConcat,), (Dilyara,), (GetResult,)]\n",
    "# pipeline_0 = Pipeline(neuro_pipe, mode='infer', output='dataset')\n",
    "# neurodata = pipeline_0.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 510/58780 [00:00<00:11, 5099.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Starting tokenization ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58780/58780 [00:09<00:00, 5900.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Tokenization was done. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 243/49830 [00:00<00:20, 2421.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Starting vectorization ... ]\n",
      "[ Vectorization of train part of dataset ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49830/49830 [00:21<00:00, 2370.49it/s]\n",
      "  4%|▍         | 186/4475 [00:00<00:02, 1841.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Vectorization of valid part of dataset ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:02<00:00, 1975.30it/s]\n",
      "  4%|▍         | 185/4475 [00:00<00:02, 1849.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Vectorization of test part of dataset ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:02<00:00, 2045.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Vectorization was ended. ]\n",
      "[ Initializing intent_model from scratch ]\n",
      "\n",
      "____Training over 49830 samples____\n",
      "\n",
      "\n",
      "train -->\tupdates: 1\tloss: 0.3408034145832062\tfmeasure: 0.02941175550222397\t \n",
      "train -->\tupdates: 501\tloss: 0.13379965722560883\tfmeasure: 0.6285713911056519\t \n",
      "epochs_done: 1\n",
      "train -->\tupdates: 780\tloss: 0.1236955001950264\tfmeasure: 0.6428570747375488\t \n",
      "train -->\tupdates: 1280\tloss: 0.15532679855823517\tfmeasure: 0.5656565427780151\t \n",
      "epochs_done: 2\n",
      "train -->\tupdates: 1559\tloss: 0.11766139417886734\tfmeasure: 0.7047618627548218\t \n",
      "train -->\tupdates: 2059\tloss: 0.11040987819433212\tfmeasure: 0.5858585238456726\t \n",
      "epochs_done: 3\n",
      "train -->\tupdates: 2338\tloss: 0.09717939794063568\tfmeasure: 0.7079644799232483\t \n",
      "train -->\tupdates: 2838\tloss: 0.11789267510175705\tfmeasure: 0.6285713911056519\t \n",
      "epochs_done: 4\n",
      "train -->\tupdates: 3117\tloss: 0.11459743976593018\tfmeasure: 0.7256636023521423\t \n",
      "train -->\tupdates: 3617\tloss: 0.08892318606376648\tfmeasure: 0.770642101764679\t \n",
      "epochs_done: 5\n"
     ]
    }
   ],
   "source": [
    "config = get_cnn_conf(path_1, emb_path)\n",
    "WCNN = GetCNN(CNN, config)\n",
    "neuro_pipe = [(Tokenizer, ), (FasttextVectorizer,), (WCNN,)]\n",
    "pipeline_1 = Pipeline(neuro_pipe, mode='train', output=None)\n",
    "neurodata = pipeline_1.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline_1.get_last_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2910: DtypeWarning: Columns (6,7,10,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  2%|▏         | 1128/58780 [00:00<00:10, 5631.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Starting tokenization ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58780/58780 [00:09<00:00, 6066.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Tokenization was done. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 229/49830 [00:00<00:22, 2248.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Starting vectorization ... ]\n",
      "[ Vectorization of train part of dataset ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49830/49830 [00:20<00:00, 2461.85it/s]\n",
      "  5%|▌         | 225/4475 [00:00<00:01, 2246.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Vectorization of valid part of dataset ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:02<00:00, 2074.34it/s]\n",
      "  4%|▍         | 187/4475 [00:00<00:02, 1862.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Vectorization of test part of dataset ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:02<00:00, 2017.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Vectorization was ended. ]\n"
     ]
    }
   ],
   "source": [
    "pipe_ = [(Tokenizer, ), (FasttextVectorizer,)]\n",
    "pipe = Pipeline(pipe_)\n",
    "\n",
    "dataset = init_dataset_tiny(file_path, language, dataset_name, date)\n",
    "data = pipe.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<script.core.dataset.Watcher at 0x7f17a4eb4390>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid', 'test', 'train_vec', 'valid_vec', 'test_vec', 'predicted_test'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_data.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 17)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_data.data['predicted_test'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00670387, 0.01195906, 0.21842773, 0.11691485, 0.00080399,\n",
       "       0.00406716, 0.5889138 , 0.00868505, 0.00401698, 0.00794192,\n",
       "       0.01409593, 0.00502214, 0.0046623 , 0.00147805, 0.00124307,\n",
       "       0.00431627, 0.0007478 ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_data.data['predicted_test'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5889138"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_data.data['predicted_test'][0][0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax = np.argmax(pred_data.data['predicted_test'][0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  1,  0,  2,  0,  6,  5,  1,  5,  9,  1, 10,  5,  9,  1,  0,  5,\n",
       "        1, 10,  1,  5,  5,  1, 11, 10,  5,  5,  5,  5,  5,  5,  5,  1,  2,\n",
       "        9, 11, 10,  1, 10, 12,  5,  1,  5, 14, 12,  1,  2,  2, 11, 11,  0,\n",
       "        1,  6,  2, 12,  5,  9,  2, 11, 15,  2,  5,  5,  5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  2,  1,  7,  1,  7,  3,  2, 12, 10,  3, 14,  6, 10,  3,  1,  6,\n",
       "        2, 14,  2, 15,  6,  3, 12, 10,  6,  3,  6,  6,  6,  6,  6,  2,  7,\n",
       "       10, 12, 11,  2, 11, 13,  6,  2,  3, 15, 13,  2,  9,  3, 12, 12,  1,\n",
       "        2,  7, 10, 13,  6, 10,  3, 12, 16,  9,  6,  6,  6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pred_data.data['test']['report'][:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_test\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "class GetResult(BaseTransformer):\n",
    "    def __init__(self, config=None):\n",
    "        if config is None:\n",
    "            self.config = {'op_type': 'transformer',\n",
    "                           'name': 'Resulter',\n",
    "                           'request_names': ['predicted_test'],\n",
    "                           'new_names': ['test']}\n",
    "        else:\n",
    "            self.config = config\n",
    "\n",
    "        super().__init__(self.config)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        request, report = dataset.main_names\n",
    "\n",
    "        pred_name = self.config['request_names'][0]\n",
    "        print(pred_name)\n",
    "        real_name = self.config['new_names'][0]\n",
    "        print(real_name)\n",
    "        pred_data = dataset.data[pred_name]\n",
    "        real_data = np.array(dataset.data[real_name][report])\n",
    "\n",
    "        preds = pred_data[0]\n",
    "        for x in pred_data[1:]:\n",
    "            preds = np.concatenate((preds, x), axis=0)\n",
    "\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        for i, x in enumerate(preds):\n",
    "            preds[i] = x + 1\n",
    "\n",
    "        preds = preds[:len(real_data)]\n",
    "\n",
    "        results = get_result(preds, real_data)\n",
    "        dataset.data['results'] = results\n",
    "\n",
    "        conf = dataset.pipeline_config\n",
    "        date = dataset.date\n",
    "        logging(results, conf, date)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "end_pipe = Pipeline([(GetResult,)])\n",
    "end_res = end_pipe.run(pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 'accuracy : 0.7640223463687151',\n",
       " 'classes': {'1': {'f1': 0.0,\n",
       "   'number_test_objects': 0,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0},\n",
       "  '10': {'f1': 0.35398230088495575,\n",
       "   'number_test_objects': 90,\n",
       "   'precision': 0.8695652173913043,\n",
       "   'recall': 0.2222222222222222},\n",
       "  '11': {'f1': 0.8212121212121212,\n",
       "   'number_test_objects': 349,\n",
       "   'precision': 0.8713826366559485,\n",
       "   'recall': 0.7765042979942693},\n",
       "  '12': {'f1': 0.6717171717171717,\n",
       "   'number_test_objects': 203,\n",
       "   'precision': 0.689119170984456,\n",
       "   'recall': 0.6551724137931034},\n",
       "  '13': {'f1': 0.7402597402597402,\n",
       "   'number_test_objects': 220,\n",
       "   'precision': 0.7066115702479339,\n",
       "   'recall': 0.7772727272727272},\n",
       "  '14': {'f1': 0.7837445573294629,\n",
       "   'number_test_objects': 345,\n",
       "   'precision': 0.7848837209302325,\n",
       "   'recall': 0.782608695652174},\n",
       "  '15': {'f1': 0.0,\n",
       "   'number_test_objects': 20,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0},\n",
       "  '16': {'f1': 0.8380952380952381,\n",
       "   'number_test_objects': 118,\n",
       "   'precision': 0.9565217391304348,\n",
       "   'recall': 0.7457627118644068},\n",
       "  '17': {'f1': 0.7623762376237624,\n",
       "   'number_test_objects': 85,\n",
       "   'precision': 0.6581196581196581,\n",
       "   'recall': 0.9058823529411765},\n",
       "  '2': {'f1': 0.877310924369748,\n",
       "   'number_test_objects': 327,\n",
       "   'precision': 0.9738805970149254,\n",
       "   'recall': 0.7981651376146789},\n",
       "  '3': {'f1': 0.7648221343873518,\n",
       "   'number_test_objects': 529,\n",
       "   'precision': 0.8012422360248447,\n",
       "   'recall': 0.7315689981096408},\n",
       "  '4': {'f1': 0.6682215743440233,\n",
       "   'number_test_objects': 678,\n",
       "   'precision': 0.5525554484088717,\n",
       "   'recall': 0.8451327433628318},\n",
       "  '5': {'f1': 0.0,\n",
       "   'number_test_objects': 136,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0},\n",
       "  '6': {'f1': 0.0, 'number_test_objects': 8, 'precision': 0.0, 'recall': 0.0},\n",
       "  '7': {'f1': 0.895895895895896,\n",
       "   'number_test_objects': 958,\n",
       "   'precision': 0.8605769230769231,\n",
       "   'recall': 0.9342379958246346},\n",
       "  '8': {'f1': 0.7959183673469388,\n",
       "   'number_test_objects': 361,\n",
       "   'precision': 0.84,\n",
       "   'recall': 0.7562326869806094},\n",
       "  '9': {'f1': 0.0,\n",
       "   'number_test_objects': 44,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0}},\n",
       " 'confusion_matrix': [[261, 10, 44, 0, 0, 4, 4, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0],\n",
       "  [1, 387, 113, 0, 0, 18, 3, 0, 0, 1, 0, 2, 4, 0, 0, 0, 0],\n",
       "  [4, 58, 573, 0, 0, 22, 12, 0, 0, 3, 3, 0, 3, 0, 0, 0, 0],\n",
       "  [0, 7, 120, 0, 0, 1, 6, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 3, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 12, 13, 0, 0, 895, 4, 0, 0, 3, 1, 17, 6, 0, 3, 4, 0],\n",
       "  [1, 2, 52, 0, 0, 5, 273, 0, 1, 6, 11, 4, 6, 0, 0, 0, 0],\n",
       "  [1, 2, 22, 0, 0, 3, 4, 0, 1, 0, 10, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 52, 0, 0, 8, 2, 0, 20, 0, 5, 0, 2, 0, 0, 0, 0],\n",
       "  [0, 0, 22, 0, 0, 17, 5, 0, 0, 271, 22, 2, 10, 0, 0, 0, 0],\n",
       "  [0, 1, 19, 0, 0, 11, 10, 0, 1, 20, 133, 4, 3, 0, 1, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 16, 0, 0, 0, 0, 0, 171, 29, 0, 0, 3, 0],\n",
       "  [0, 1, 2, 0, 0, 5, 1, 0, 0, 4, 1, 28, 270, 0, 0, 33, 0],\n",
       "  [0, 0, 1, 0, 0, 5, 0, 0, 0, 0, 4, 9, 1, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 22, 1, 0, 0, 2, 1, 0, 3, 0, 88, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 5, 0, 0, 77, 0],\n",
       "  [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]],\n",
       " 'f1_macro': 0.5278562507921417,\n",
       " 'f1_weighted': 0.7467903607409845}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_res.data['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = get_cnn_conf(path_1, emb_path)\n",
    "# dataset = init_dataset()\n",
    "# WCNN = GetCNN(CNN, config)\n",
    "# neuro_pipe = [(Tokenizer, ), (FasttextVectorizer,), (WCNN,), (GetResult,)]\n",
    "# pipeline_1 = Pipeline(neuro_pipe, mode='infer', output='dataset')\n",
    "# neurodata = pipeline_1.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = get_cnn_conf(path_1, emb_path)\n",
    "# dataset = init_dataset()\n",
    "# dCNN = GetCNN(DCNN, config)\n",
    "# neuro_pipe = [(Tokenizer, ), (FasttextVectorizer,), (dCNN,), (GetResult,)]\n",
    "# pipeline_2 = Pipeline(neuro_pipe, mode='infer', output='dataset')\n",
    "# neurodata = pipeline_2.run(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines with Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_0 = {'op_type': 'model', 'name': 'Linear Regression',\n",
    "          'fit_names': ['train_vec'], 'new_names': ['predicted_test'],\n",
    "          'predict_names': ['test_vec']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline with linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Tokenizer, tok_conf), (Lemmatizer,), (concat, None),\n",
    "# dataset = init_dataset()\n",
    "# LR = skmodel(LogisticRegression, conf_0)\n",
    "\n",
    "# pipe_0 = [(tfidf_, tfidf_conf_2), (LR,), (GetResultLinear_W,)]\n",
    "# pipeline_0 = Pipeline(pipe_0, mode='train', output=None)\n",
    "# pipeline_1 = Pipeline(pipe_0, mode='infer', output='dataset')\n",
    "# res = pipeline_1.run(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline with SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = init_dataset()\n",
    "\n",
    "# conf_0['name'] = 'LinearSVC'\n",
    "# conf_0['op_type'] = 'model'\n",
    "# LSVC = skmodel(LinearSVC, conf_0)\n",
    "\n",
    "# # (Speller, spl_conf),\n",
    "\n",
    "# pipe_1 = [(Tokenizer, ), (Lemmatizer,), (concat, None), (tfidf_, tfidf_conf_2),\n",
    "#           (LSVC,), (GetResultLinear_W,)]\n",
    "# pipeline_2 = Pipeline(pipe_1, mode='train', output=None)\n",
    "# pipeline_3 = Pipeline(pipe_1, mode='infer', output='dataset')\n",
    "\n",
    "# res = pipeline_3.run(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Run pipeline with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = init_dataset()\n",
    "\n",
    "# conf_0['name'] = 'RandomForestClassifier'\n",
    "# conf_0['op_type'] = 'model'\n",
    "# RFC = skmodel(RandomForestClassifier, conf_0)\n",
    "\n",
    "\n",
    "# # (Speller, spl_conf), \n",
    "# pipe_2 = [(Tokenizer, ), (Lemmatizer,), (concat, None), (tfidf_, tfidf_conf_2),\n",
    "#           (RFC,), (GetResultLinear_W,)]\n",
    "# pipeline_4 = Pipeline(pipe_2, mode='train', output=None)\n",
    "# pipeline_5 = Pipeline(pipe_2, mode='infer', output='dataset')\n",
    "\n",
    "# res = pipeline_5.run(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline with GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os.path import join\n",
    "\n",
    "# date = datetime.datetime.now()\n",
    "# dataset_name = 'vkusvill'\n",
    "# language = 'russian'\n",
    "# file_path = join('./data', language, dataset_name, 'data', 'vkusvill_all_categories.csv')\n",
    "\n",
    "# dataset = init_dataset_tiny(file_path, language, dataset_name, date)\n",
    "\n",
    "# conf_0['name'] = 'LGBMClassifier'\n",
    "# conf_0['op_type'] = 'model'\n",
    "# LGBM = skmodel(LGBMClassifier, conf_0)\n",
    "\n",
    "# # (Speller, spl_conf),\n",
    "# pipe_3 = [(Tokenizer, ), (Lemmatizer,), (concat, None), (tfidf_, ),\n",
    "#           (LGBM,), (GetResultLinear_W,)]\n",
    "# pipeline_6 = Pipeline(pipe_3, mode='train', output=None)\n",
    "# pipeline_7 = Pipeline(pipe_3, mode='infer', output='dataset')\n",
    "\n",
    "# res = pipeline_7.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
