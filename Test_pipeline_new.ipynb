{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-03 18:30:49.209 DEBUG in 'matplotlib.backends'['__init__'] at line 90: backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from os.path import join\n",
    "\n",
    "from script.core.transformers import *\n",
    "from script.models.cnn import CNN\n",
    "from script.core.models import *\n",
    "from script.core.dataset import Watcher\n",
    "from script.core.utils import read_dataset, get_result, logging\n",
    "from script.core.pipeline import Pipeline\n",
    "\n",
    "# linear models\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# sklearn feachure extractors\n",
    "from sklearn.feature_extraction.text import CountVectorizer as count\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Pure DilyaraModel\n",
    "# from intent_classifier.intent_model.model_wrap import KerasMulticlassModel as DilyaraModel\n",
    "# from script.models.cnn import CNN\n",
    "# from script.models.dcnn import DCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read csv file and create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2910: DtypeWarning: Columns (6,7,10,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "def init_dataset_tiny(file_path, language, dataset_name, date, seed=42):\n",
    "    pure_data = read_dataset(file_path, True, True)  # It not default meanings!!!\n",
    "    start_dataset = Watcher(pure_data, date, language, dataset_name,\n",
    "                            seed=seed)  # classes_descriptions = {} we can do it\n",
    "\n",
    "    ######################################################################################\n",
    "    dataset = start_dataset.split([0.1, 0.1])\n",
    "    data = dataset.data['test']\n",
    "    start_dataset = Watcher(data, date, language, dataset_name, seed)\n",
    "    ######################################################################################\n",
    "\n",
    "    return start_dataset\n",
    "\n",
    "date = datetime.datetime.now()\n",
    "dataset_name = 'vkusvill'\n",
    "language = 'russian'\n",
    "file_path = join('./data', language, dataset_name, 'data', 'vkusvill_all_categories.csv')\n",
    "\n",
    "dataset = init_dataset_tiny(file_path, language, dataset_name, date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_conf = {'op_type': 'transformer',\n",
    "            'name': 'Speller',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base'],\n",
    "            'path': './DeepPavlov/deeppavlov/configs/error_model/brillmoore_kartaslov_ru.json'}\n",
    "\n",
    "tok_conf = {'op_type': 'transformer',\n",
    "            'name': 'Tokenizer',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base']}\n",
    "\n",
    "lem_conf = {'op_type': 'transformer',\n",
    "            'name': 'Lemmatizer',\n",
    "            'request_names': ['base'],\n",
    "            'new_names': ['base']}\n",
    "\n",
    "concat = TextConcat()\n",
    "\n",
    "tfidf_conf_1 = {'op_type': 'vectorizer', 'name': 'tf-idf vectorizer',\n",
    "                'request_names': ['train', 'valid', 'test'], 'new_names': ['train_vec', 'valid_vec', 'test_vec']}\n",
    "tfidf_conf_2 = {'op_type': 'vectorizer', 'name': 'count_vectorizer',\n",
    "                'request_names': ['train', 'valid', 'test'], 'new_names': ['train_vec', 'valid_vec', 'test_vec']}\n",
    "tfidf_ = sktransformer(tfidf, tfidf_conf_1)\n",
    "count_ = sktransformer(count, tfidf_conf_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'в магазине 1018м мыт сукромка7 место выкладки теплые стеллажи нонфуд зафиксирована температура ниже 18 температура на начало дня 16 3температура в середине дня 16 4 температура на конец дня 16 1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data['base']['request'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_cnn_conf(path_1, emb_path)\n",
    "WCNN = GetCNN(CNN, config)\n",
    "pipe = Pipeline([(Tokenizer,), (FasttextVectorizer,), (WCNN,)])\n",
    "pipe.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.load_data('38b2f8a46a974b45d9eaa3b76f304a60')\n",
    "dataset.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['в',\n",
       " 'магазине',\n",
       " '1018м',\n",
       " 'мыт',\n",
       " 'сукромка7',\n",
       " 'место',\n",
       " 'выкладки',\n",
       " 'теплые',\n",
       " 'стеллажи',\n",
       " 'нонфуд',\n",
       " 'зафиксирована',\n",
       " 'температура',\n",
       " 'ниже',\n",
       " '18',\n",
       " 'температура',\n",
       " 'на',\n",
       " 'начало',\n",
       " 'дня',\n",
       " '16',\n",
       " '3температура',\n",
       " 'в',\n",
       " 'середине',\n",
       " 'дня',\n",
       " '16',\n",
       " '4',\n",
       " 'температура',\n",
       " 'на',\n",
       " 'конец',\n",
       " 'дня',\n",
       " '16',\n",
       " '1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data['base']['request'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = dataset.data['base']['request'][0]\n",
    "# s = s[1:-1]\n",
    "# s = s.split(', ')\n",
    "# s = [x[1:-1] for x in s]\n",
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = lambda s: [x[1:-1] for x in s[1:-1].split(', ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data['base']['request'] = dataset.data['base']['request'].apply(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data['base']['request'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_cnn_conf(path_1, emb_path)\n",
    "WCNN = GetCNN(CNN, config)\n",
    "pipe = Pipeline([(FasttextVectorizer,), (WCNN,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 181/3661 [00:00<00:01, 1802.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Starting vectorization ... ]\n",
      "[ Vectorization of train part of dataset ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3661/3661 [00:01<00:00, 1971.09it/s]\n",
      " 43%|████▎     | 177/407 [00:00<00:00, 1765.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Vectorization of valid part of dataset ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407/407 [00:00<00:00, 1867.32it/s]\n",
      " 43%|████▎     | 173/407 [00:00<00:00, 1723.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Vectorization of test part of dataset ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407/407 [00:00<00:00, 1758.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Vectorization was ended. ]\n",
      "[ Initializing intent_model from scratch ]\n",
      "\n",
      "____Training over 3661 samples____\n",
      "\n",
      "\n",
      "train -->\tupdates: 1\tloss: 0.358571857213974\tfmeasure: 0.0\t \n",
      "epochs_done: 1\n",
      "train -->\tupdates: 59\tloss: 0.23870240151882172\tfmeasure: 0.42696624994277954\t \n",
      "epochs_done: 2\n",
      "train -->\tupdates: 117\tloss: 0.19705097377300262\tfmeasure: 0.5416666269302368\t \n",
      "epochs_done: 3\n",
      "train -->\tupdates: 175\tloss: 0.18926946818828583\tfmeasure: 0.54347825050354\t \n",
      "epochs_done: 4\n",
      "train -->\tupdates: 233\tloss: 0.17898912727832794\tfmeasure: 0.5154638886451721\t \n",
      "epochs_done: 5\n"
     ]
    }
   ],
   "source": [
    "pipe.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline with Neural Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetCNN(BaseModel):\n",
    "    def init_model(self, dataset):\n",
    "        classes = dataset.get_classes()\n",
    "        classes = ' '.join([str(x) for x in classes])\n",
    "        self.config['classes'] = classes\n",
    "        \n",
    "        super().init_model(dataset)\n",
    "        \n",
    "        return self\n",
    "\n",
    "def get_cnn_conf(path, emb_path, fit_names=None, predict_names=None, new_names=None):\n",
    "    with open(path, 'r') as conf:\n",
    "        config = json.load(conf)\n",
    "\n",
    "    config['op_type'] = 'model'\n",
    "    config['name'] = 'cnn'\n",
    "    \n",
    "    if fit_names is not None:\n",
    "        config['fit_names'] = fit_names\n",
    "    else:\n",
    "        config['fit_names'] = ['train_vec']\n",
    "    \n",
    "    if predict_names is not None:\n",
    "        config['predict_names'] = predict_names\n",
    "    else:\n",
    "        config['predict_names'] = ['test_vec']\n",
    "        \n",
    "    if new_names is not None:\n",
    "        config['new_names'] = new_names\n",
    "    else:\n",
    "        config['new_names'] = ['predicted_test']\n",
    "    \n",
    "    config['fasttext_model'] = emb_path\n",
    "    \n",
    "    return config\n",
    "\n",
    "path_0 = './configs/models/CNN/CNN_opt.json'\n",
    "path_1 = './configs/models/CNN.json'\n",
    "emb_path = './data/russian/embeddings/ft_0.8.3_nltk_yalen_sg_300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = get_cnn_conf(path_0, emb_path, fit_names=['train'], predict_names=['test'])\n",
    "# dataset = init_dataset()\n",
    "# Dilyara = GetCNN(DilyaraModel, config)\n",
    "# neuro_pipe = [(Tokenizer, ), (Lemmatizer,), (TextConcat,), (Dilyara,), (GetResult,)]\n",
    "# pipeline_0 = Pipeline(neuro_pipe, mode='infer', output='dataset')\n",
    "# neurodata = pipeline_0.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_cnn_conf(path_1, emb_path)\n",
    "WCNN = GetCNN(CNN, config)\n",
    "neuro_pipe = [(Tokenizer, ), (FasttextVectorizer,), (WCNN,)]\n",
    "pipeline_1 = Pipeline(neuro_pipe, mode='train', output=None)\n",
    "neurodata = pipeline_1.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline_1.get_last_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe_ = [(Tokenizer, ), (FasttextVectorizer,)]\n",
    "# pipe = Pipeline(pipe_)\n",
    "\n",
    "# dataset = init_dataset_tiny(file_path, language, dataset_name, date)\n",
    "data = model.predict(dataset, predict_name='test_vec', new_name='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data.data['pred'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data.data['predicted_test'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data.data['predicted_test'][0][0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax = np.argmax(pred_data.data['predicted_test'][0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(pred_data.data['test']['report'][:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetResult(BaseTransformer):\n",
    "    def __init__(self, config=None):\n",
    "        if config is None:\n",
    "            self.config = {'op_type': 'transformer',\n",
    "                           'name': 'Resulter',\n",
    "                           'request_names': ['predicted_test'],\n",
    "                           'new_names': ['test']}\n",
    "        else:\n",
    "            self.config = config\n",
    "\n",
    "        super().__init__(self.config)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        request, report = dataset.main_names\n",
    "\n",
    "        pred_name = self.config['request_names'][0]\n",
    "        print(pred_name)\n",
    "        real_name = self.config['new_names'][0]\n",
    "        print(real_name)\n",
    "        pred_data = dataset.data[pred_name]\n",
    "        real_data = np.array(dataset.data[real_name][report])\n",
    "\n",
    "        preds = pred_data[0]\n",
    "        for x in pred_data[1:]:\n",
    "            preds = np.concatenate((preds, x), axis=0)\n",
    "\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        for i, x in enumerate(preds):\n",
    "            preds[i] = x + 1\n",
    "\n",
    "        preds = preds[:len(real_data)]\n",
    "\n",
    "        results = get_result(preds, real_data)\n",
    "        dataset.data['results'] = results\n",
    "\n",
    "        conf = dataset.pipeline_config\n",
    "        date = dataset.date\n",
    "        logging(results, conf, date)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "end_pipe = Pipeline([(GetResult,)])\n",
    "end_res = end_pipe.run(pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_res.data['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = get_cnn_conf(path_1, emb_path)\n",
    "# dataset = init_dataset()\n",
    "# WCNN = GetCNN(CNN, config)\n",
    "# neuro_pipe = [(Tokenizer, ), (FasttextVectorizer,), (WCNN,), (GetResult,)]\n",
    "# pipeline_1 = Pipeline(neuro_pipe, mode='infer', output='dataset')\n",
    "# neurodata = pipeline_1.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = get_cnn_conf(path_1, emb_path)\n",
    "# dataset = init_dataset()\n",
    "# dCNN = GetCNN(DCNN, config)\n",
    "# neuro_pipe = [(Tokenizer, ), (FasttextVectorizer,), (dCNN,), (GetResult,)]\n",
    "# pipeline_2 = Pipeline(neuro_pipe, mode='infer', output='dataset')\n",
    "# neurodata = pipeline_2.run(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines with Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_0 = {'op_type': 'model', 'name': 'Linear Regression',\n",
    "          'fit_names': ['train_vec'], 'new_names': ['predicted_test'],\n",
    "          'predict_names': ['test_vec']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline with linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Tokenizer, tok_conf), (Lemmatizer,), (concat, None),\n",
    "# dataset = init_dataset()\n",
    "# LR = skmodel(LogisticRegression, conf_0)\n",
    "\n",
    "# pipe_0 = [(tfidf_, tfidf_conf_2), (LR,), (GetResultLinear_W,)]\n",
    "# pipeline_0 = Pipeline(pipe_0, mode='train', output=None)\n",
    "# pipeline_1 = Pipeline(pipe_0, mode='infer', output='dataset')\n",
    "# res = pipeline_1.run(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline with SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = init_dataset()\n",
    "\n",
    "# conf_0['name'] = 'LinearSVC'\n",
    "# conf_0['op_type'] = 'model'\n",
    "# LSVC = skmodel(LinearSVC, conf_0)\n",
    "\n",
    "# # (Speller, spl_conf),\n",
    "\n",
    "# pipe_1 = [(Tokenizer, ), (Lemmatizer,), (concat, None), (tfidf_, tfidf_conf_2),\n",
    "#           (LSVC,), (GetResultLinear_W,)]\n",
    "# pipeline_2 = Pipeline(pipe_1, mode='train', output=None)\n",
    "# pipeline_3 = Pipeline(pipe_1, mode='infer', output='dataset')\n",
    "\n",
    "# res = pipeline_3.run(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Run pipeline with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = init_dataset()\n",
    "\n",
    "# conf_0['name'] = 'RandomForestClassifier'\n",
    "# conf_0['op_type'] = 'model'\n",
    "# RFC = skmodel(RandomForestClassifier, conf_0)\n",
    "\n",
    "\n",
    "# # (Speller, spl_conf), \n",
    "# pipe_2 = [(Tokenizer, ), (Lemmatizer,), (concat, None), (tfidf_, tfidf_conf_2),\n",
    "#           (RFC,), (GetResultLinear_W,)]\n",
    "# pipeline_4 = Pipeline(pipe_2, mode='train', output=None)\n",
    "# pipeline_5 = Pipeline(pipe_2, mode='infer', output='dataset')\n",
    "\n",
    "# res = pipeline_5.run(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline with GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os.path import join\n",
    "\n",
    "# date = datetime.datetime.now()\n",
    "# dataset_name = 'vkusvill'\n",
    "# language = 'russian'\n",
    "# file_path = join('./data', language, dataset_name, 'data', 'vkusvill_all_categories.csv')\n",
    "\n",
    "# dataset = init_dataset_tiny(file_path, language, dataset_name, date)\n",
    "\n",
    "# conf_0['name'] = 'LGBMClassifier'\n",
    "# conf_0['op_type'] = 'model'\n",
    "# LGBM = skmodel(LGBMClassifier, conf_0)\n",
    "\n",
    "# # (Speller, spl_conf),\n",
    "# pipe_3 = [(Tokenizer, ), (Lemmatizer,), (concat, None), (tfidf_, ),\n",
    "#           (LGBM,), (GetResultLinear_W,)]\n",
    "# pipeline_6 = Pipeline(pipe_3, mode='train', output=None)\n",
    "# pipeline_7 = Pipeline(pipe_3, mode='infer', output='dataset')\n",
    "\n",
    "# res = pipeline_7.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
