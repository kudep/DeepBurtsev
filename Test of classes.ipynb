{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "2018-03-21 15:08:56.676 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 16: Loading dictionaries from /home/mks/envs/intent_script/lib/python3.6/site-packages/pymorphy2_dicts/data\n",
      "2018-03-21 15:08:56.712 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 20: format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n",
      "2018-03-21 15:08:56.725 DEBUG in 'matplotlib.backends'['__init__'] at line 90: backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pymorphy2\n",
    "import fasttext\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import labels2onehot_one\n",
    "from deeppavlov.core.commands.infer import build_model_from_config\n",
    "from dataset import Dataset\n",
    "from transformer import Speller, Tokenizer, Lemmatizer, FasttextVectorizer\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# linear models\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# sklearn feachure extractors\n",
    "from sklearn.feature_extraction.text import CountVectorizer as count\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import random\n",
    "from typing import Generator\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filepath, duplicates=False, clean=True):\n",
    "    file = open(filepath, 'r', encoding='ISO-8859-1')\n",
    "    data = pd.read_csv(file)\n",
    "\n",
    "    old_names = data.keys()\n",
    "    names = [n.encode('ISO-8859-1').decode('cp1251').encode('utf8') for n in old_names]\n",
    "    names = [n.decode('utf-8') for n in names]\n",
    "\n",
    "    new_data = dict()\n",
    "    for old, new in zip(old_names, names):\n",
    "        new_data[new] = list()\n",
    "        for c in data[old]:\n",
    "            try:\n",
    "                s = c.encode('ISO-8859-1').decode('cp1251').encode('utf8')\n",
    "                s = s.decode('utf-8')\n",
    "                new_data[new].append(s)\n",
    "            except AttributeError:\n",
    "                new_data[new].append(c)\n",
    "\n",
    "    new_data = pd.DataFrame(new_data, columns=['Описание', 'Категория жалобы'])\n",
    "    new_data.rename(columns={'Описание': 'request', 'Категория жалобы': 'report'}, inplace=True)\n",
    "    new_data = new_data.dropna()  # dell nan\n",
    "    if not duplicates:\n",
    "        new_data = new_data.drop_duplicates()  # dell duplicates\n",
    "\n",
    "    # как отдельную ветвь можно использовать\n",
    "    if clean:\n",
    "        delete_bad_symbols = lambda x: \" \".join(re.sub('[^а-яa-zё0-9]', ' ', x.lower()).split())\n",
    "        new_data['request'] = new_data['request'].apply(delete_bad_symbols)\n",
    "\n",
    "    new_data = new_data.reset_index()\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, data, seed=None, classes_description=None, *args, **kwargs):\n",
    "\n",
    "        self.main_names = ['request', 'report']\n",
    "\n",
    "        rs = random.getstate()\n",
    "        random.seed(seed)\n",
    "        self.random_state = random.getstate()\n",
    "        random.setstate(rs)\n",
    "\n",
    "        self.classes_description = classes_description\n",
    "        self.data = dict()\n",
    "\n",
    "        if data.get('train') is not None:\n",
    "            self.data['train'] = data.get('train')\n",
    "        elif data.get('test') is not None:\n",
    "            self.data['test'] = data.get('test')\n",
    "        elif data.get('valid') is not None:\n",
    "            self.data['valid'] = data.get('valid')\n",
    "        else:\n",
    "            self.data['base'] = data\n",
    "\n",
    "        self.classes = self.get_classes()\n",
    "        self.classes_distribution = self.get_distribution()\n",
    "\n",
    "    def simple_split(self, splitting_proportions, field_to_split, splitted_fields, delete_parent=True):\n",
    "        data_to_div = self.data[field_to_split].copy()\n",
    "        data_size = len(self.data[field_to_split])\n",
    "        for i in range(len(splitted_fields) - 1):\n",
    "            self.data[splitted_fields[i]], data_to_div = train_test_split(data_to_div,\n",
    "                                                                          test_size=\n",
    "                                                                          len(data_to_div) -\n",
    "                                                                          int(data_size * splitting_proportions[i]))\n",
    "        self.data[splitted_fields[-1]] = data_to_div\n",
    "\n",
    "        if delete_parent:\n",
    "            a = self.data.pop(field_to_split)\n",
    "            del a\n",
    "\n",
    "        return self\n",
    "\n",
    "    def split(self, splitting_proportions=None, delete_parent=True):\n",
    "\n",
    "        dd = dict()\n",
    "        cd = self.classes_distribution\n",
    "        train = list()\n",
    "        valid = list()\n",
    "        test = list()\n",
    "\n",
    "        if splitting_proportions is None:\n",
    "            splitting_proportions = [0.1, 0.1]\n",
    "\n",
    "        if self.data.get('base', []) is not None:\n",
    "            dataset = self.data['base']\n",
    "        else:\n",
    "            raise ValueError(\"You dataset don't contains 'base' key. If You want to split a specific part dataset,\"\n",
    "                             \"please use .simple_split method.\")\n",
    "\n",
    "        for x, y in zip(dataset[self.main_names[0]], dataset[self.main_names[1]]):\n",
    "            if y not in dd.keys():\n",
    "                dd[y] = list()\n",
    "                dd[y].append((x, y))\n",
    "            else:\n",
    "                dd[y].append((x, y))\n",
    "\n",
    "        if type(splitting_proportions) is list:\n",
    "            assert len(splitting_proportions) == 2\n",
    "            assert type(splitting_proportions[0]) is float\n",
    "\n",
    "            valid_ = dict()\n",
    "            test_ = dict()\n",
    "\n",
    "            for x in dd.keys():\n",
    "                num = int(cd[x] * splitting_proportions[0])\n",
    "                valid_[x] = random.sample(dd[x], num)\n",
    "                [dd[x].remove(t) for t in valid_[x]]\n",
    "\n",
    "            for x in dd.keys():\n",
    "                num = int(cd[x] * splitting_proportions[1])\n",
    "                test_[x] = random.sample(dd[x], num)\n",
    "                [dd[x].remove(t) for t in test_[x]]\n",
    "        else:\n",
    "            raise ValueError('Split proportion must be list of floats, with length = 2')\n",
    "\n",
    "        train_ = dd\n",
    "\n",
    "        for x in train_.keys():\n",
    "            for z_, z in zip([train_, valid_, test_], [train, valid, test]):\n",
    "                z.extend(z_[x])\n",
    "\n",
    "        del train_, valid_, test_, dd, cd, dataset\n",
    "\n",
    "        for z in [train, valid, test]:\n",
    "            z = random.shuffle(z)\n",
    "\n",
    "        utrain, uvalid, utest, ctrain, cvalid, ctest = list(), list(), list(), list(), list(), list()\n",
    "        for z, n, c in zip([train, valid, test], [utrain, uvalid, utest], [ctrain, cvalid, ctest]):\n",
    "            for x in z:\n",
    "                n.append(x[0])\n",
    "                c.append(x[1])\n",
    "\n",
    "        self.data['train'] = pd.DataFrame({self.main_names[0]: utrain, self.main_names[1]: ctrain})\n",
    "        self.data['valid'] = pd.DataFrame({self.main_names[0]: uvalid, self.main_names[1]: cvalid})\n",
    "        self.data['test'] = pd.DataFrame({self.main_names[0]: utest, self.main_names[1]: ctest})\n",
    "\n",
    "        if delete_parent:\n",
    "            a = self.data.pop('base', [])\n",
    "            del a\n",
    "\n",
    "        return self\n",
    "\n",
    "    def iter_batch(self, batch_size: int, data_type: str = 'base') -> Generator:\n",
    "        \"\"\"This function returns a generator, which serves for generation of raw (no preprocessing such as tokenization)\n",
    "         batches\n",
    "        Args:\n",
    "            batch_size (int): number of samples in batch\n",
    "            data_type (str): can be either 'train', 'test', or 'valid'\n",
    "        Returns:\n",
    "            batch_gen (Generator): a generator, that iterates through the part (defined by data_type) of the dataset\n",
    "        \"\"\"\n",
    "        data = self.data[data_type]\n",
    "        data_len = len(data)\n",
    "        order = list(range(data_len))\n",
    "\n",
    "        rs = random.getstate()\n",
    "        random.setstate(self.random_state)\n",
    "        random.shuffle(order)\n",
    "        self.random_state = random.getstate()\n",
    "        random.setstate(rs)\n",
    "\n",
    "        # for i in range((data_len - 1) // batch_size + 1):\n",
    "        #     yield list(zip(*[data[o] for o in order[i * batch_size:(i + 1) * batch_size]]))\n",
    "        for i in range((data_len - 1) // batch_size + 1):\n",
    "            o = order[i * batch_size:(i + 1) * batch_size]\n",
    "            yield list((list(data[self.main_names[0]][o]), list(data[self.main_names[1]][o])))\n",
    "\n",
    "    def iter_all(self, data_type: str = 'base') -> Generator:\n",
    "        \"\"\"\n",
    "        Iterate through all data. It can be used for building dictionary or\n",
    "        Args:\n",
    "            data_type (str): can be either 'train', 'test', or 'valid'\n",
    "        Returns:\n",
    "            samples_gen: a generator, that iterates through the all samples in the selected data type of the dataset\n",
    "        \"\"\"\n",
    "        data = self.data[data_type]\n",
    "        for x, y in zip(data[self.main_names[0]], data[self.main_names[1]]):\n",
    "            yield (x, y)\n",
    "\n",
    "    def merge_data(self, fields_to_merge, delete_parent=True, new_name=None):\n",
    "        if new_name is None:\n",
    "            new_name = '_'.join([s for s in fields_to_merge])\n",
    "\n",
    "        if set(fields_to_merge) <= set(self.data.keys()):\n",
    "            fraims_to_merge = [self.data[s] for s in fields_to_merge]\n",
    "            self.data[new_name] = pd.concat(fraims_to_merge)\n",
    "        else:\n",
    "            raise KeyError('In dataset no such parts {}'.format(fields_to_merge))\n",
    "\n",
    "        if delete_parent:\n",
    "            a = [self.data.pop(x) for x in fields_to_merge]\n",
    "            del a\n",
    "\n",
    "        return self\n",
    "\n",
    "    def del_data(self, fields_to_del):\n",
    "        for name in fields_to_del:\n",
    "            a = self.data.pop(name)\n",
    "            del a\n",
    "        return self\n",
    "\n",
    "    def get_classes(self):\n",
    "        if self.data.get('base') is not None:\n",
    "            classes = self.data['base'][self.main_names[1]].unique()\n",
    "        else:\n",
    "            classes = self.data['train'][self.main_names[1]].unique()\n",
    "        return classes\n",
    "\n",
    "    def get_distribution(self):\n",
    "        try:\n",
    "            classes_distribution = self.data['base'].groupby(self.main_names[1])[self.main_names[0]].nunique()\n",
    "        except KeyError:\n",
    "            classes_distribution = self.data['train'].groupby(self.main_names[1])[self.main_names[0]].nunique()\n",
    "        return classes_distribution\n",
    "\n",
    "    def info(self):\n",
    "        information = dict(data_keys=list(self.data.keys()),\n",
    "                           classes_description=self.classes_description)\n",
    "\n",
    "        return information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2850: DtypeWarning: Columns (6,7,10,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "path = '/home/mks/projects/intent_classification_script/data/russian/data/vkusvill_all_categories.csv'\n",
    "global_data = read_dataset(path)\n",
    "dataset = Dataset(global_data, seed=42)\n",
    "# dataset = dataset.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    здравствуйте сегодня попались стекляшки в кака...\n",
       "1              попаля кусок стекла отнесут магазин 295\n",
       "2    уважаемый вкусвилл купила на той неделе варень...\n",
       "3    купила 17 07 котлеты лососевые от 21 06 три уп...\n",
       "4    офигеть купил во вкусвилле бутылку облепиховог...\n",
       "Name: request, dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data['base']['request'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTransformer(object):\n",
    "    def __init__(self, info=None, request_names=None, new_names=None):\n",
    "        # info resist\n",
    "        if isinstance(info, list):\n",
    "            self.info = {'op_type': info[0], 'name': info[1]}\n",
    "        elif isinstance(info, dict):\n",
    "#             if set(['op_type', 'name']) in info.keys():\n",
    "            if ('op_type' in info.keys()) and ('name' in info.keys()):\n",
    "                self.info = info\n",
    "            else:\n",
    "                raise ValueError('Attribute info dict must contain fields \"op_type\" and \"name\",'\n",
    "                                 'but {} was found.'.format(info.keys()))\n",
    "        elif info is None:\n",
    "            self.info = {'op_type': 'transformer', 'name': 'op_'}\n",
    "        else:\n",
    "            raise ValueError('Attribute info must be list, dict or None, but {} was found.'.format(type(info)))\n",
    "        \n",
    "        # named spaces\n",
    "        self.new_names = new_names\n",
    "        self.worked_names = request_names\n",
    "        self.request_names = []\n",
    "    \n",
    "    def _validate_names(self, dataset):\n",
    "        if self.worked_names is not None:\n",
    "            if not isinstance(self.worked_names, list):\n",
    "                raise ValueError('Request_names must be a list, but {} was found.'.format(type(self.worked_names)))\n",
    "            \n",
    "            for name in self.worked_names:\n",
    "                if name not in dataset.data.keys():\n",
    "                    raise KeyError('Key {} not found in dataset.'.format(name))\n",
    "                else:\n",
    "                    self.request_names.append(name)\n",
    "        else:\n",
    "            self.worked_names = ['base', 'train', 'valid', 'test']\n",
    "            for name in self.worked_names:\n",
    "                if name in dataset.data.keys():\n",
    "                    self.request_names.append(name)\n",
    "            if len(self.request_names) == 0:\n",
    "                raise KeyError('Keys from {} not found in dataset.'.format(self.worked_names))\n",
    "        \n",
    "        if self.new_names is None:\n",
    "            self.new_names = self.request_names\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataset):\n",
    "        self._validate_names(dataset)\n",
    "        return self._transform(dataset)\n",
    "        \n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "\n",
    "    def set_params(self, params):\n",
    "        # self.params = params\n",
    "        self.__init__(params)\n",
    "        return self\n",
    "\n",
    "\n",
    "class Speller(BaseTransformer):\n",
    "    def __init__(self, params=None, info=None, request_names=None, new_names=None):\n",
    "        super().__init__(info, request_names, new_names)\n",
    "        \n",
    "        if params is None:\n",
    "            self.conf_path = '/home/mks/projects/intent_classification_script/DeepPavlov/deeppavlov/configs/error_model/brillmoore_kartaslov_ru.json'\n",
    "        else:\n",
    "            if isinstance(params, dict):\n",
    "                self.conf_path = params['path']\n",
    "            else:\n",
    "                raise ValueError('Attribute params must be dict, but {} was found.'.format(type(params)))\n",
    "\n",
    "        with open(self.conf_path) as config_file:\n",
    "            self.config = json.load(config_file)\n",
    "\n",
    "        self.speller = build_model_from_config(self.config)\n",
    "\n",
    "    def _transform(self, dataset): \n",
    "        print('[ Speller start working ... ]')\n",
    "        \n",
    "        request, report = dataset.main_names\n",
    "        for name, new_name in zip(self.request_names, self.new_names):\n",
    "            data = dataset.data[name]\n",
    "            refactor = list()\n",
    "            \n",
    "            for x in tqdm(data[request]):\n",
    "                refactor.append(self.speller([x])[0])\n",
    "            \n",
    "            dataset.data[new_name] = pd.DataFrame({request: refactor,\n",
    "                                                   report: data[report]})\n",
    "        \n",
    "        print('[ Speller done. ]')\n",
    "        return dataset\n",
    "\n",
    "\n",
    "class Tokenizer(BaseTransformer):\n",
    "    def __init__(self, params=None, info=None, request_names=None, new_names=None):\n",
    "        self.params = params\n",
    "        super().__init__(info, request_names, new_names)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        print('[ Starting tokenization ... ]')\n",
    "        \n",
    "        request, report = dataset.main_names        \n",
    "        for name, new_name in zip(self.request_names, self.new_names):\n",
    "            data = dataset.data[name][request]\n",
    "            tok_data = list()\n",
    "            \n",
    "            for x in tqdm(data):\n",
    "                sent_toks = nltk.sent_tokenize(x)\n",
    "                word_toks = [nltk.word_tokenize(el) for el in sent_toks]\n",
    "                tokens = [val for sublist in word_toks for val in sublist]\n",
    "                tok_data.append(tokens)\n",
    "\n",
    "            dataset.data[new_name] = pd.DataFrame({request: tok_data,\n",
    "                                                   report: dataset.data[name][report]})\n",
    "        \n",
    "        print('[ Tokenization was done. ]')\n",
    "        return dataset\n",
    "\n",
    "\n",
    "class Lemmatizer(BaseTransformer):\n",
    "    def __init__(self, params=None, info=None, request_names=None, new_names=None):\n",
    "        self.params = params\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        super().__init__(info, request_names, new_names)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        print('[ Starting lemmatization ... ]')\n",
    "        request, report = dataset.main_names\n",
    "        for name, new_name in zip(self.request_names, self.new_names):\n",
    "            data = dataset.data[name][request]\n",
    "            morph_data = list()\n",
    "            \n",
    "            for x in tqdm(data):\n",
    "                mp_data = [self.morph.parse(el)[0].normal_form for el in x]\n",
    "                morph_data.append(mp_data)\n",
    "\n",
    "            dataset.data[new_name] = pd.DataFrame({request: morph_data,\n",
    "                                                   report: dataset.data[name][report]})\n",
    "        print('[ Ended lemmatization. ]')\n",
    "        return dataset\n",
    "\n",
    "\n",
    "class FasttextVectorizer(BaseTransformer):\n",
    "    def __init__(self, params=None, info=None, request_names=None, new_names=None):\n",
    "        super().__init__(info, request_names, new_names)\n",
    "        self.info['op_type'] = 'vectorizer'\n",
    "        \n",
    "#         print(type(self.new_names))\n",
    "#         for i, name in enumerate(self.new_names):\n",
    "#             name = name + '_' + 'vec'\n",
    "#             self.new_names[i] = name\n",
    "        \n",
    "        if params is None:\n",
    "            self.params = {'path_to_model': '/home/mks/projects/intent_classification_script/data/russian/embeddings/ft_0.8.3_nltk_yalen_sg_300.bin',\n",
    "                           'dimension': 300,\n",
    "                           'file_type': 'bin'}\n",
    "\n",
    "        self.vectorizer = fasttext.load_model(self.params['path_to_model'])\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        print('[ Starting vectorization ... ]')\n",
    "        request, report = dataset.main_names\n",
    "        \n",
    "        for name, new_name in zip(self.request_names, self.new_names):\n",
    "            print('[ Vectorization of {} part of dataset ... ]'.format(name))\n",
    "            data = dataset.data[name][request]\n",
    "            vec_request = []\n",
    "            \n",
    "            for x in tqdm(data):\n",
    "                matrix_i = np.zeros((len(x), self.params['dimension']))\n",
    "                for j, y in enumerate(x):\n",
    "                    matrix_i[j] = self.vectorizer[y]\n",
    "                vec_request.append(matrix_i)\n",
    "\n",
    "            vec_report = list(labels2onehot_one(dataset.data[name][report], dataset.classes))\n",
    "\n",
    "            dataset.data[new_name] = pd.DataFrame({request: vec_request,\n",
    "                                                   report: vec_report})\n",
    "        \n",
    "        print('[ Vectorization was ended. ]')\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = BaseTransformer(info=['transformer', 'estomator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.BaseTransformer at 0x7fc1bf5166d8>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a._validate_names(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid', 'test'])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.split()\n",
    "dataset.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-21 17:43:01.788 INFO in 'deeppavlov.vocabs.typos'['typos'] at line 76: Loading a dictionary from /home/mks/projects/intent_classification_script/DeepPavlov/download/russian_words_vocab\n",
      "2018-03-21 17:43:05.685 INFO in 'deeppavlov.models.spellers.error_model.error_model'['error_model'] at line 239: loading error_model from `/home/mks/projects/intent_classification_script/DeepPavlov/download/error_model/error_model_ru.tsv`\n",
      "2018-03-21 17:43:06.280 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 16: Loading dictionaries from /home/mks/envs/intent_script/lib/python3.6/site-packages/pymorphy2_dicts/data\n",
      "2018-03-21 17:43:06.305 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 20: format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    }
   ],
   "source": [
    "req = ['test']\n",
    "\n",
    "speller = Speller(info=['transformer', 'Speller'], request_names=req)\n",
    "lemma = Lemmatizer(info=['transformer', 'Lemmatizator'], request_names=req)\n",
    "tokenizer = Tokenizer(info=['transformer', 'Tokenizator'], request_names=req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 955/4475 [00:00<00:00, 4773.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Starting tokenization ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:00<00:00, 5073.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Tokenization was done. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_ = tokenizer.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report</th>\n",
       "      <th>request</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>[здравствуйте, выражаю, искреннюю, благодарнос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>[томаты, розовые, в, четверг, 03, 11, моя, мам...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[списание, по, качеству, партия, от, 28, 07, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>[г, москва, ул, коломенская, д, 7, стр, 2, в, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>[привезите, пожалуйста, тофу, с, чесноком, и, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   report                                            request\n",
       "0       6  [здравствуйте, выражаю, искреннюю, благодарнос...\n",
       "1       3  [томаты, розовые, в, четверг, 03, 11, моя, мам...\n",
       "2       1  [списание, по, качеству, партия, от, 28, 07, 2...\n",
       "3       6  [г, москва, ул, коломенская, д, 7, стр, 2, в, ...\n",
       "4      10  [привезите, пожалуйста, тофу, с, чесноком, и, ..."
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_.data['test'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastvec = FasttextVectorizer(info=['vectorizer', 'fasttext'], request_names=req,\n",
    "                             new_names=['test_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 118/4475 [00:00<00:03, 1167.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Starting vectorization ... ]\n",
      "[ Vectorization of test part of dataset ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:02<00:00, 1723.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Vectorization was ended. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_ = fastvec.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test']"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastvec.request_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_vec']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastvec.new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid', 'test', 'test_vec'])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_.data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skwrapper(BaseTransformer):\n",
    "    def __init__(self, t, info=None, request_names=None, new_names=None):\n",
    "        super().__init__(info, request_names, new_names)\n",
    "        \n",
    "        if (not hasattr(t, \"fit\") or not (hasattr(t, \"fit_transform\")) or hasattr(t, \"transform\"))):\n",
    "            raise TypeError(\"Methods fit, fit_transform, transform are not implemented in class {} \"\n",
    "                            \"Sklearn transformers and estimators shoud implement fit and transform.\"\n",
    "                            \" '%s' (type %s) doesn't\" % (t, type(t)))\n",
    "        \n",
    "        self.transformer = t\n",
    "        self.trained = False\n",
    "\n",
    "\n",
    "class sktransformer(skwrapper):  \n",
    "    def __init__(self, t, info=None, request_names=None, new_names=None):\n",
    "        super().__init__(t, info, request_names, new_names)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        request, report = dataset.main_names\n",
    "        if hasattr(self.transformer, 'fit_transform') and not self.trained:\n",
    "            if 'base' not in dataset.data.keys():\n",
    "                dataset.merge_data(fields_to_merge=self.request_names, delete_parent=False, new_name='base')\n",
    "                X = dataset.data['base'][request]\n",
    "                y = dataset.data['base'][report]\n",
    "                # fit \n",
    "                self.transformer.fit(X, y)\n",
    "                self.trained = True\n",
    "\n",
    "                # delete 'base' from dataset\n",
    "                dataset.del_data(['base'])\n",
    "            else:\n",
    "                X = dataset.data['base'][request]\n",
    "                y = dataset.data['base'][report]\n",
    "                # fit \n",
    "                self.transformer.fit(X, y)\n",
    "                self.trained = True\n",
    "            \n",
    "            # transform all fields\n",
    "            for name, new_name in zip(self.request_names, self.new_names):\n",
    "                X = dataset.data[name][request]\n",
    "                y = dataset.data[name][report]            \n",
    "                dataset.data[new_name] = {request: self.transformer.transform(X),\n",
    "                                          report: y}\n",
    "                \n",
    "        else:     \n",
    "            for name, new_name in zip(self.request_names, self.new_names):\n",
    "                X = dataset.data[name][request]\n",
    "                y = dataset.data[name][report]            \n",
    "                dataset.data[new_name] = {request: self.transformer.transform(X),\n",
    "                                          report: y}\n",
    "            \n",
    "        return dataset\n",
    "\n",
    "    \n",
    "class skmodel(skwrapper):\n",
    "    def __init__(self, t, info=None, request_names=None, new_names=None):\n",
    "        super().__init__(t, info, request_names, new_names)\n",
    "    \n",
    "    def fit(self, dataset):\n",
    "        request, report = dataset.main_names\n",
    "        \n",
    "        if 'train_vec' in dataset.data.keys():\n",
    "            name = 'train_vec'\n",
    "        else:\n",
    "            if 'train' in dataset.data.keys():\n",
    "                name = 'train'\n",
    "            else:\n",
    "                raise KeyError('Dataset must contain \"train_vec\" or \"train\" fields.')\n",
    "            \n",
    "        X = dataset.data[name][request]\n",
    "        y = dataset.data[name][report]\n",
    "\n",
    "        if hasattr(self.transformer, 'fit') and not hasattr(self.transformer, 'fit_tranform'):\n",
    "            self.transformer.fit(X, y)\n",
    "            self.trained = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, dataset, request_names=None, new_names=None):\n",
    "        \n",
    "        if not hasattr(self.transformer, 'predict'):\n",
    "            raise TypeError(\"Methods predict, is not implemented in class {} \"\n",
    "                            \" '%s' (type %s) doesn't\" % (self.transformer, type(self.transformer)))\n",
    "        \n",
    "        request, report = dataset.main_names\n",
    "        \n",
    "        if not self.trained:\n",
    "            # TODO write correct error\n",
    "            raise ValueError('Sklearn model is not trained yet.')\n",
    "        \n",
    "        if (request_names is not None) and (new_names):\n",
    "            self.request_names = request_names\n",
    "            self.new_names = new_names\n",
    "        \n",
    "        for name, new_name in zip(self.request_names, self.new_names):\n",
    "            X = dataset.data[name][request]\n",
    "            dataset.data[new_name] = self.transformer.predict(X)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def fit_predict(self, dataset, request_names=None, new_names=None):      \n",
    "        self.fit(dataset)\n",
    "        dataset = self.predict(dataset, request_names, new_names)\n",
    "        return dataset\n",
    "    \n",
    "    def predict_data(self, dataset, request_names=None, new_names=None):\n",
    "        \n",
    "        if not hasattr(self.transformer, 'predict'):\n",
    "            raise TypeError(\"Methods predict, is not implemented in class {} \"\n",
    "                            \" '%s' (type %s) doesn't\" % (self.transformer, type(self.transformer)))\n",
    "        \n",
    "        request, report = dataset.main_names\n",
    "        \n",
    "        if not self.trained:\n",
    "            # TODO write correct error\n",
    "            raise ValueError('Sklearn model is not trained yet.')\n",
    "        \n",
    "        if (request_names is not None) and (new_names):\n",
    "            self.request_names = request_names\n",
    "            self.new_names = new_names\n",
    "        \n",
    "        res = []\n",
    "        for name, new_name in zip(self.request_names, self.new_names):\n",
    "            X = dataset.data[name][request]\n",
    "            res.append(self.transformer.predict(X))\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def fit_predict_data(self, dataset, request_names=None, new_names=None):      \n",
    "        self.fit(dataset)\n",
    "        res = self.predict_data(dataset, request_names, new_names)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "info1 = {'op_type': 'vectorizer', 'name': 'tf-idf vectorizer'}\n",
    "info2 = {'op_type': 'model', 'name': 'Linear Regression'}\n",
    "\n",
    "clf = tfidf()\n",
    "lr = LogisticRegression()\n",
    "\n",
    "my_vec = sktransformer(clf, info=info1,\n",
    "                       request_names=['train', 'valid', 'test'],\n",
    "                       new_names=['train_vec', 'valid_vec', 'test_vec'])\n",
    "my_lr = skmodel(lr, info=info2)\n",
    "\n",
    "dataset.split()\n",
    "\n",
    "dataset_ = my_vec.transform(dataset)\n",
    "dataset_ = my_lr.fit(dataset_).predict(dataset_,\n",
    "                                       request_names=['test_vec', 'valid_vec'],\n",
    "                                       new_names=['test_new', 'valid_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid', 'test', 'train_vec', 'valid_vec', 'test_vec', 'test_new', 'valid_new'])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(name, config=None):\n",
    "    names = set(['speller', 'lemmatizer', 'tokenizer', 'fasttext_vectorizer',\n",
    "                 'count_vectorizer', 'tf-idf'])\n",
    "    \n",
    "    if name not in names:\n",
    "        raise TypeError('{} is not implemented.'.format(name))\n",
    "    \n",
    "    if name == 'speller':\n",
    "        return Speller(config)\n",
    "    elif name == 'lemmatizer':\n",
    "        return Lemmatizer(config)\n",
    "    elif name == 'tokenizer':\n",
    "        return Tokenizer(config)\n",
    "    elif name == 'fasttext_vectorizer':\n",
    "        return FasttextVectorizer(config)\n",
    "    elif name == 'tf-idf':\n",
    "        return sktransformer(TfidfVectorizer(config))\n",
    "    elif name == 'count_vectorizer':\n",
    "        return sktransformer(CountVectorizer(config))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline(object):\n",
    "    def __init__(self, pipe):\n",
    "        self.pipe = pipe\n",
    "    \n",
    "    def fit(self, dataset):\n",
    "\n",
    "        for op in self.pipe:\n",
    "            operation = op[1]\n",
    "            if operation is not None:\n",
    "                if operation.info['op_type'] == 'transformer':\n",
    "                    dataset = operation.transform(dataset)\n",
    "                elif operation.info['op_type'] == 'vectorizer':\n",
    "                    if 'train' not in dataset.data.keys():\n",
    "                        dataset.split()\n",
    "                    operation.transform(dataset)\n",
    "                elif operation.info['op_type'] == 'model':\n",
    "                    operation.fit(dataset)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        print('[ Train End. ]')\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        prediction = None\n",
    "\n",
    "        for op in self.pipe:\n",
    "            operation = op[1]\n",
    "            if operation is not None:\n",
    "                if operation.info['op_type'] == 'transformer':\n",
    "                    dataset = operation.transform(dataset)\n",
    "                elif operation.info['op_type'] == 'vectorizer':\n",
    "                    if 'train' not in dataset.data.keys():\n",
    "                        dataset.split()\n",
    "                    operation.transform(dataset)\n",
    "                elif operation.info['op_type'] == 'model':\n",
    "                    prediction = operation.predict(dataset)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        print('[ Prediction End. ]')\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def run(self, fit_dataset, predict_dataset=None):\n",
    "        self.fit(fit_dataset)\n",
    "        if predict_dataset is None:\n",
    "            prediction = self.predict(fit_dataset)\n",
    "        else:\n",
    "            prediction = self.predict(predict_dataset)\n",
    "        \n",
    "        print('[ End. ]')\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2850: DtypeWarning: Columns (6,7,10,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "path = '/home/mks/projects/intent_classification_script/data/russian/data/vkusvill_all_categories.csv'\n",
    "global_data = read_dataset(path)\n",
    "dataset = Dataset(global_data, seed=42)\n",
    "dataset = dataset.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test'])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data['train'] = dataset.data['valid']\n",
    "dataset.del_data(['valid'])\n",
    "dataset.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = ['train', 'test']\n",
    "info1 = {'op_type': 'vectorizer', 'name': 'tf-idf vectorizer'}\n",
    "info2 = {'op_type': 'model', 'name': 'Linear Regression'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tfidf()\n",
    "lr = LogisticRegression()\n",
    "\n",
    "my_vec = sktransformer(clf, info=info1,\n",
    "                       request_names=req,\n",
    "                       new_names=['train_vec', 'test_vec'])\n",
    "my_lr = skmodel(lr, info=info2, request_names=req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-21 18:20:34.411 INFO in 'deeppavlov.vocabs.typos'['typos'] at line 76: Loading a dictionary from /home/mks/projects/intent_classification_script/DeepPavlov/download/russian_words_vocab\n",
      "2018-03-21 18:20:38.513 INFO in 'deeppavlov.models.spellers.error_model.error_model'['error_model'] at line 239: loading error_model from `/home/mks/projects/intent_classification_script/DeepPavlov/download/error_model/error_model_ru.tsv`\n",
      "2018-03-21 18:20:39.93 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 16: Loading dictionaries from /home/mks/envs/intent_script/lib/python3.6/site-packages/pymorphy2_dicts/data\n",
      "2018-03-21 18:20:39.118 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 20: format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    }
   ],
   "source": [
    "speller = Speller(info=['transformer', 'Speller'], request_names=req)\n",
    "lemma = Lemmatizer(info=['transformer', 'Lemmatizator'], request_names=req)\n",
    "tokenizer = Tokenizer(info=['transformer', 'Tokenizator'], request_names=req)\n",
    "fastvec = FasttextVectorizer(info=['vectorizer', 'fasttext'], request_names=req,\n",
    "                             new_names=['train_vec', 'test_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = [('tf-idf', my_vec), ('LR', my_lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train End. ]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zip argument #2 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-283-39fae280dd56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-267-cac006874c22>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0moperation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'op_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-131-79a58c69f1ee>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, dataset, request_names, new_names)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: zip argument #2 must support iteration"
     ]
    }
   ],
   "source": [
    "pipeline = pipeline.fit(dataset).predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_.data['test'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tiny test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base'])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_test_data = pd.DataFrame({'request': ['Это тут', \"типо такой\", \"тест офигенный\"],\n",
    "                               'report': [0, 1, 2]})\n",
    "tiny_test_dataset = Dataset(tiny_test_data)\n",
    "tiny_test_dataset.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 230.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Speller start working ... ]\n",
      "[ Speller done. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = speller.transform(tiny_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 7049.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Starting tokenization ... ]\n",
      "[ Tokenization was done. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = tokenizer.transform(tiny_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1261.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Starting lemmatization ... ]\n",
      "[ Ended lemmatization. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = lemma.transform(tiny_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Starting vectorization ... ]\n",
      "[ Vectorization was ended. ]\n"
     ]
    }
   ],
   "source": [
    "# tiny_dataset.split()\n",
    "tiny_dataset = fastvec.transform(tiny_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base'])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_dataset.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report</th>\n",
       "      <th>request</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[это, тут]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[тип, такой]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[тест, офигенный]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   report            request\n",
       "0       0         [это, тут]\n",
       "1       1       [тип, такой]\n",
       "2       2  [тест, офигенный]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_dataset.data['base']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
