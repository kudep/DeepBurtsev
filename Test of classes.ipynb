{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "2018-03-22 22:31:18.863 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 16: Loading dictionaries from /home/mks/envs/intent_script/lib/python3.6/site-packages/pymorphy2_dicts/data\n",
      "2018-03-22 22:31:18.910 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 20: format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n",
      "2018-03-22 22:31:19.4 DEBUG in 'matplotlib.backends'['__init__'] at line 90: backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pymorphy2\n",
    "import fasttext\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import labels2onehot_one\n",
    "from deeppavlov.core.commands.infer import build_model_from_config\n",
    "from dataset import Dataset\n",
    "from transformer import Speller, Tokenizer, Lemmatizer, FasttextVectorizer\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# linear models\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# sklearn feachure extractors\n",
    "from sklearn.feature_extraction.text import CountVectorizer as count\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import random\n",
    "from typing import Generator\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filepath, duplicates=False, clean=True):\n",
    "    file = open(filepath, 'r', encoding='ISO-8859-1')\n",
    "    data = pd.read_csv(file)\n",
    "\n",
    "    old_names = data.keys()\n",
    "    names = [n.encode('ISO-8859-1').decode('cp1251').encode('utf8') for n in old_names]\n",
    "    names = [n.decode('utf-8') for n in names]\n",
    "\n",
    "    new_data = dict()\n",
    "    for old, new in zip(old_names, names):\n",
    "        new_data[new] = list()\n",
    "        for c in data[old]:\n",
    "            try:\n",
    "                s = c.encode('ISO-8859-1').decode('cp1251').encode('utf8')\n",
    "                s = s.decode('utf-8')\n",
    "                new_data[new].append(s)\n",
    "            except AttributeError:\n",
    "                new_data[new].append(c)\n",
    "\n",
    "    new_data = pd.DataFrame(new_data, columns=['Описание', 'Категория жалобы'])\n",
    "    new_data.rename(columns={'Описание': 'request', 'Категория жалобы': 'report'}, inplace=True)\n",
    "    new_data = new_data.dropna()  # dell nan\n",
    "    if not duplicates:\n",
    "        new_data = new_data.drop_duplicates()  # dell duplicates\n",
    "\n",
    "    # как отдельную ветвь можно использовать\n",
    "    if clean:\n",
    "        delete_bad_symbols = lambda x: \" \".join(re.sub('[^а-яa-zё0-9]', ' ', x.lower()).split())\n",
    "        new_data['request'] = new_data['request'].apply(delete_bad_symbols)\n",
    "\n",
    "    new_data = new_data.reset_index()\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, data, seed=None, classes_description=None, *args, **kwargs):\n",
    "\n",
    "        self.main_names = ['request', 'report']\n",
    "\n",
    "        rs = random.getstate()\n",
    "        random.seed(seed)\n",
    "        self.random_state = random.getstate()\n",
    "        random.setstate(rs)\n",
    "\n",
    "        self.classes_description = classes_description\n",
    "        self.data = dict()\n",
    "\n",
    "        if data.get('train') is not None:\n",
    "            self.data['train'] = data.get('train')\n",
    "        elif data.get('test') is not None:\n",
    "            self.data['test'] = data.get('test')\n",
    "        elif data.get('valid') is not None:\n",
    "            self.data['valid'] = data.get('valid')\n",
    "        else:\n",
    "            self.data['base'] = data\n",
    "\n",
    "        self.classes = self.get_classes()\n",
    "        self.classes_distribution = self.get_distribution()\n",
    "\n",
    "    def simple_split(self, splitting_proportions, field_to_split, splitted_fields, delete_parent=True):\n",
    "        data_to_div = self.data[field_to_split].copy()\n",
    "        data_size = len(self.data[field_to_split])\n",
    "        for i in range(len(splitted_fields) - 1):\n",
    "            self.data[splitted_fields[i]], data_to_div = train_test_split(data_to_div,\n",
    "                                                                          test_size=\n",
    "                                                                          len(data_to_div) -\n",
    "                                                                          int(data_size * splitting_proportions[i]))\n",
    "        self.data[splitted_fields[-1]] = data_to_div\n",
    "\n",
    "        if delete_parent:\n",
    "            a = self.data.pop(field_to_split)\n",
    "            del a\n",
    "\n",
    "        return self\n",
    "\n",
    "    def split(self, splitting_proportions=None, delete_parent=True):\n",
    "\n",
    "        dd = dict()\n",
    "        cd = self.classes_distribution\n",
    "        train = list()\n",
    "        valid = list()\n",
    "        test = list()\n",
    "\n",
    "        if splitting_proportions is None:\n",
    "            splitting_proportions = [0.1, 0.1]\n",
    "\n",
    "        if self.data.get('base', []) is not None:\n",
    "            dataset = self.data['base']\n",
    "        else:\n",
    "            raise ValueError(\"You dataset don't contains 'base' key. If You want to split a specific part dataset,\"\n",
    "                             \"please use .simple_split method.\")\n",
    "\n",
    "        for x, y in zip(dataset[self.main_names[0]], dataset[self.main_names[1]]):\n",
    "            if y not in dd.keys():\n",
    "                dd[y] = list()\n",
    "                dd[y].append((x, y))\n",
    "            else:\n",
    "                dd[y].append((x, y))\n",
    "\n",
    "        if type(splitting_proportions) is list:\n",
    "            assert len(splitting_proportions) == 2\n",
    "            assert type(splitting_proportions[0]) is float\n",
    "\n",
    "            valid_ = dict()\n",
    "            test_ = dict()\n",
    "\n",
    "            for x in dd.keys():\n",
    "                num = int(cd[x] * splitting_proportions[0])\n",
    "                valid_[x] = random.sample(dd[x], num)\n",
    "                [dd[x].remove(t) for t in valid_[x]]\n",
    "\n",
    "            for x in dd.keys():\n",
    "                num = int(cd[x] * splitting_proportions[1])\n",
    "                test_[x] = random.sample(dd[x], num)\n",
    "                [dd[x].remove(t) for t in test_[x]]\n",
    "        else:\n",
    "            raise ValueError('Split proportion must be list of floats, with length = 2')\n",
    "\n",
    "        train_ = dd\n",
    "\n",
    "        for x in train_.keys():\n",
    "            for z_, z in zip([train_, valid_, test_], [train, valid, test]):\n",
    "                z.extend(z_[x])\n",
    "\n",
    "        del train_, valid_, test_, dd, cd, dataset\n",
    "\n",
    "        for z in [train, valid, test]:\n",
    "            z = random.shuffle(z)\n",
    "\n",
    "        utrain, uvalid, utest, ctrain, cvalid, ctest = list(), list(), list(), list(), list(), list()\n",
    "        for z, n, c in zip([train, valid, test], [utrain, uvalid, utest], [ctrain, cvalid, ctest]):\n",
    "            for x in z:\n",
    "                n.append(x[0])\n",
    "                c.append(x[1])\n",
    "\n",
    "        self.data['train'] = pd.DataFrame({self.main_names[0]: utrain, self.main_names[1]: ctrain})\n",
    "        self.data['valid'] = pd.DataFrame({self.main_names[0]: uvalid, self.main_names[1]: cvalid})\n",
    "        self.data['test'] = pd.DataFrame({self.main_names[0]: utest, self.main_names[1]: ctest})\n",
    "\n",
    "        if delete_parent:\n",
    "            a = self.data.pop('base', [])\n",
    "            del a\n",
    "\n",
    "        return self\n",
    "\n",
    "    def iter_batch(self, batch_size: int, data_type: str = 'base') -> Generator:\n",
    "        \"\"\"This function returns a generator, which serves for generation of raw (no preprocessing such as tokenization)\n",
    "         batches\n",
    "        Args:\n",
    "            batch_size (int): number of samples in batch\n",
    "            data_type (str): can be either 'train', 'test', or 'valid'\n",
    "        Returns:\n",
    "            batch_gen (Generator): a generator, that iterates through the part (defined by data_type) of the dataset\n",
    "        \"\"\"\n",
    "        data = self.data[data_type]\n",
    "        data_len = len(data)\n",
    "        order = list(range(data_len))\n",
    "\n",
    "        rs = random.getstate()\n",
    "        random.setstate(self.random_state)\n",
    "        random.shuffle(order)\n",
    "        self.random_state = random.getstate()\n",
    "        random.setstate(rs)\n",
    "\n",
    "        # for i in range((data_len - 1) // batch_size + 1):\n",
    "        #     yield list(zip(*[data[o] for o in order[i * batch_size:(i + 1) * batch_size]]))\n",
    "        for i in range((data_len - 1) // batch_size + 1):\n",
    "            o = order[i * batch_size:(i + 1) * batch_size]\n",
    "            yield list((list(data[self.main_names[0]][o]), list(data[self.main_names[1]][o])))\n",
    "\n",
    "    def iter_all(self, data_type: str = 'base') -> Generator:\n",
    "        \"\"\"\n",
    "        Iterate through all data. It can be used for building dictionary or\n",
    "        Args:\n",
    "            data_type (str): can be either 'train', 'test', or 'valid'\n",
    "        Returns:\n",
    "            samples_gen: a generator, that iterates through the all samples in the selected data type of the dataset\n",
    "        \"\"\"\n",
    "        data = self.data[data_type]\n",
    "        for x, y in zip(data[self.main_names[0]], data[self.main_names[1]]):\n",
    "            yield (x, y)\n",
    "\n",
    "    def merge_data(self, fields_to_merge, delete_parent=True, new_name=None):\n",
    "        if new_name is None:\n",
    "            new_name = '_'.join([s for s in fields_to_merge])\n",
    "\n",
    "        if set(fields_to_merge) <= set(self.data.keys()):\n",
    "            fraims_to_merge = [self.data[s] for s in fields_to_merge]\n",
    "            self.data[new_name] = pd.concat(fraims_to_merge)\n",
    "        else:\n",
    "            raise KeyError('In dataset no such parts {}'.format(fields_to_merge))\n",
    "\n",
    "        if delete_parent:\n",
    "            a = [self.data.pop(x) for x in fields_to_merge]\n",
    "            del a\n",
    "\n",
    "        return self\n",
    "\n",
    "    def del_data(self, fields_to_del):\n",
    "        for name in fields_to_del:\n",
    "            a = self.data.pop(name)\n",
    "            del a\n",
    "        return self\n",
    "\n",
    "    def get_classes(self):\n",
    "        if self.data.get('base') is not None:\n",
    "            classes = self.data['base'][self.main_names[1]].unique()\n",
    "        else:\n",
    "            classes = self.data['train'][self.main_names[1]].unique()\n",
    "        return classes\n",
    "\n",
    "    def get_distribution(self):\n",
    "        try:\n",
    "            classes_distribution = self.data['base'].groupby(self.main_names[1])[self.main_names[0]].nunique()\n",
    "        except KeyError:\n",
    "            classes_distribution = self.data['train'].groupby(self.main_names[1])[self.main_names[0]].nunique()\n",
    "        return classes_distribution\n",
    "\n",
    "    def info(self):\n",
    "        information = dict(data_keys=list(self.data.keys()),\n",
    "                           classes_description=self.classes_description)\n",
    "\n",
    "        return information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2850: DtypeWarning: Columns (6,7,10,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "path = '/home/mks/projects/intent_classification_script/data/russian/data/vkusvill_all_categories.csv'\n",
    "global_data = read_dataset(path)\n",
    "dataset = Dataset(global_data, seed=42)\n",
    "dataset = dataset.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTransformer(object):\n",
    "    def __init__(self, config=None):\n",
    "        # info resist\n",
    "        if not isinstance(config, dict):\n",
    "            raise ValueError('Input config must be dict or None, but {} was found.'.format(type(config)))\n",
    "\n",
    "        keys = ['op_type', 'name', 'request_names', 'new_names', 'input_x_type', 'input_y_type', 'output_x_type',\n",
    "                'output_y_type']\n",
    "        self.info = dict()\n",
    "        for x in keys:\n",
    "            if x not in config.keys():\n",
    "                raise ValueError('Input config must contain {} key.'.format(x))\n",
    "            self.info[x] = config[x]\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # named spaces\n",
    "        self.new_names = config['new_names']\n",
    "        self.worked_names = config['request_names']\n",
    "        self.request_names = []\n",
    "\n",
    "    def _validate_names(self, dataset):\n",
    "        if self.worked_names is not None:\n",
    "            if not isinstance(self.worked_names, list):\n",
    "                raise ValueError('Request_names must be a list, but {} was found.'.format(type(self.worked_names)))\n",
    "\n",
    "            for name in self.worked_names:\n",
    "                if name not in dataset.data.keys():\n",
    "                    raise KeyError('Key {} not found in dataset.'.format(name))\n",
    "                else:\n",
    "                    self.request_names.append(name)\n",
    "        else:\n",
    "            self.worked_names = ['base', 'train', 'valid', 'test']\n",
    "            for name in self.worked_names:\n",
    "                if name in dataset.data.keys():\n",
    "                    self.request_names.append(name)\n",
    "            if len(self.request_names) == 0:\n",
    "                raise KeyError('Keys from {} not found in dataset.'.format(self.worked_names))\n",
    "\n",
    "        if self.new_names is None:\n",
    "            self.new_names = self.request_names\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        return None\n",
    "\n",
    "    def transform(self, dataset):\n",
    "        self._validate_names(dataset)\n",
    "        return self._transform(dataset)\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.config\n",
    "\n",
    "    def set_params(self, params):\n",
    "        # self.params = params\n",
    "        self.__init__(params)\n",
    "        return self\n",
    "\n",
    "\n",
    "class Speller(BaseTransformer):\n",
    "    def __init__(self, config=None):\n",
    "        if config is None:\n",
    "            self.config = {'op_type': 'transformer',\n",
    "                           'name': 'Speller',\n",
    "                           'request_names': ['base'],\n",
    "                           'new_names': ['base'],\n",
    "                           'input_x_type': pd.core.series.Series,\n",
    "                           'input_y_type': pd.core.series.Series,\n",
    "                           'output_x_type': pd.core.series.Series,\n",
    "                           'output_y_type': pd.core.series.Series,\n",
    "                           'path': '/home/mks/projects/intent_classification_script/DeepPavlov/deeppavlov/configs/error_model/brillmoore_kartaslov_ru.json'}\n",
    "        else:\n",
    "            need_names = ['path']\n",
    "            for name in need_names:\n",
    "                if name not in config.keys():\n",
    "                    raise ValueError('Input config must contain {}.'.format(name))\n",
    "\n",
    "            self.config = config\n",
    "\n",
    "        super().__init__(self.config)\n",
    "\n",
    "        self.conf_path = self.config['path']\n",
    "        with open(self.conf_path) as config_file:\n",
    "            self.config = json.load(config_file)\n",
    "\n",
    "        self.speller = build_model_from_config(self.config)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        print('[ Speller start working ... ]')\n",
    "\n",
    "        request, report = dataset.main_names\n",
    "        for name, new_name in zip(self.request_names, self.new_names):\n",
    "            data = dataset.data[name]\n",
    "            refactor = list()\n",
    "\n",
    "            for x in tqdm(data[request]):\n",
    "                refactor.append(self.speller([x])[0])\n",
    "\n",
    "            dataset.data[new_name] = pd.DataFrame({request: refactor,\n",
    "                                                   report: data[report]})\n",
    "\n",
    "        print('[ Speller done. ]')\n",
    "        return dataset\n",
    "\n",
    "\n",
    "class Tokenizer(BaseTransformer):\n",
    "    def __init__(self, config=None):\n",
    "        if config is None:\n",
    "            self.config = {'op_type': 'transformer',\n",
    "                           'name': 'Tokenizer',\n",
    "                           'request_names': ['base'],\n",
    "                           'new_names': ['base'],\n",
    "                           'input_x_type': pd.core.series.Series,\n",
    "                           'input_y_type': pd.core.series.Series,\n",
    "                           'output_x_type': pd.core.series.Series,\n",
    "                           'output_y_type': pd.core.series.Series}\n",
    "        else:\n",
    "            self.config = config\n",
    "\n",
    "        super().__init__(self.config)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        print('[ Starting tokenization ... ]')\n",
    "\n",
    "        request, report = dataset.main_names\n",
    "        for name, new_name in zip(self.request_names, self.new_names):\n",
    "            data = dataset.data[name][request]\n",
    "            tok_data = list()\n",
    "\n",
    "            for x in tqdm(data):\n",
    "                sent_toks = nltk.sent_tokenize(x)\n",
    "                word_toks = [nltk.word_tokenize(el) for el in sent_toks]\n",
    "                tokens = [val for sublist in word_toks for val in sublist]\n",
    "                tok_data.append(tokens)\n",
    "\n",
    "            dataset.data[new_name] = pd.DataFrame({request: tok_data,\n",
    "                                                   report: dataset.data[name][report]})\n",
    "\n",
    "        print('[ Tokenization was done. ]')\n",
    "        return dataset\n",
    "\n",
    "\n",
    "class Lemmatizer(BaseTransformer):\n",
    "    def __init__(self, config=None):\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "        if config is None:\n",
    "            self.config = {'op_type': 'transformer',\n",
    "                           'name': 'Lemmatizer',\n",
    "                           'request_names': ['base'],\n",
    "                           'new_names': ['base'],\n",
    "                           'input_x_type': pd.core.series.Series,\n",
    "                           'input_y_type': pd.core.series.Series,\n",
    "                           'output_x_type': pd.core.series.Series,\n",
    "                           'output_y_type': pd.core.series.Series}\n",
    "        else:\n",
    "            self.config = config\n",
    "\n",
    "        super().__init__(self.config)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        print('[ Starting lemmatization ... ]')\n",
    "        request, report = dataset.main_names\n",
    "        for name, new_name in zip(self.request_names, self.new_names):\n",
    "            data = dataset.data[name][request]\n",
    "            morph_data = list()\n",
    "\n",
    "            for x in tqdm(data):\n",
    "                mp_data = [self.morph.parse(el)[0].normal_form for el in x]\n",
    "                morph_data.append(mp_data)\n",
    "\n",
    "            dataset.data[new_name] = pd.DataFrame({request: morph_data,\n",
    "                                                   report: dataset.data[name][report]})\n",
    "        print('[ Ended lemmatization. ]')\n",
    "        return dataset\n",
    "\n",
    "\n",
    "class FasttextVectorizer(BaseTransformer):\n",
    "    def __init__(self, config=None):\n",
    "\n",
    "        if config is None:\n",
    "            self.config = {'op_type': 'vectorizer',\n",
    "                           'name': 'fasttext',\n",
    "                           'request_names': ['train', 'valid', 'test'],\n",
    "                           'new_names': ['train_vec', 'valid_vec', 'test_vec'],\n",
    "                           'input_x_type': pd.core.series.Series,\n",
    "                           'input_y_type': pd.core.series.Series,\n",
    "                           'output_x_type': pd.core.series.Series,\n",
    "                           'output_y_type': pd.core.series.Series,\n",
    "                           'path_to_model': '/home/mks/projects/intent_classification_script/data/russian/embeddings/ft_0.8.3_nltk_yalen_sg_300.bin',\n",
    "                           'dimension': 300,\n",
    "                           'file_type': 'bin'}\n",
    "        else:\n",
    "            need_names = ['path_to_model', 'dimension', 'file_type']\n",
    "            for name in need_names:\n",
    "                if name not in config.keys():\n",
    "                    raise ValueError('Input config must contain {}.'.format(name))\n",
    "\n",
    "            self.config = config\n",
    "\n",
    "        super().__init__(self.config)\n",
    "\n",
    "        self.vectorizer = fasttext.load_model(self.config['path_to_model'])\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        print('[ Starting vectorization ... ]')\n",
    "        request, report = dataset.main_names\n",
    "\n",
    "        for name, new_name in zip(self.request_names, self.new_names):\n",
    "            print('[ Vectorization of {} part of dataset ... ]'.format(name))\n",
    "            data = dataset.data[name][request]\n",
    "            vec_request = []\n",
    "\n",
    "            for x in tqdm(data):\n",
    "                matrix_i = np.zeros((len(x), self.config['dimension']))\n",
    "                for j, y in enumerate(x):\n",
    "                    matrix_i[j] = self.vectorizer[y]\n",
    "                vec_request.append(matrix_i)\n",
    "\n",
    "            vec_report = list(labels2onehot_one(dataset.data[name][report], dataset.classes))\n",
    "\n",
    "            dataset.data[new_name] = pd.DataFrame({request: vec_request,\n",
    "                                                   report: vec_report})\n",
    "\n",
    "        print('[ Vectorization was ended. ]')\n",
    "        return dataset\n",
    "\n",
    "\n",
    "class TextConcat(BaseTransformer):\n",
    "    def __init__(self, config=None):\n",
    "        if config is None:\n",
    "            self.config = {'op_type': 'transformer',\n",
    "                           'name': 'text_concatenator',\n",
    "                           'request_names': ['base'],\n",
    "                           'new_names': ['base'],\n",
    "                           'input_x_type': pd.core.series.Series,\n",
    "                           'input_y_type': pd.core.series.Series,\n",
    "                           'output_x_type': pd.core.series.Series,\n",
    "                           'output_y_type': pd.core.series.Series}\n",
    "        else:\n",
    "            need_names = []\n",
    "            for name in need_names:\n",
    "                if name not in config.keys():\n",
    "                    raise ValueError('Input config must contain {}.'.format(name))\n",
    "\n",
    "            self.config = config\n",
    "\n",
    "        super().__init__(self.config)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        print('[ Starting text merging ... ]')\n",
    "        request, report = dataset.main_names\n",
    "\n",
    "        for name, new_name in zip(self.request_names, self.new_names):\n",
    "            data = dataset.data[name][request]\n",
    "            text_request = []\n",
    "\n",
    "            for x in tqdm(data):\n",
    "                text_request.append(' '.join([z for z in x]))\n",
    "\n",
    "            dataset.data[new_name] = pd.DataFrame({request: text_request,\n",
    "                                                   report: dataset.data[name][report]})\n",
    "\n",
    "        print('[ Text concatenation was ended. ]')\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = BaseTransformer({'op_type': 'transformer', 'name': 'estomator',\n",
    "                     'request_names': ['base'], 'new_names': ['base'],\n",
    "                     'input_x_type': pd.core.series.Series,\n",
    "                     'input_y_type': pd.core.series.Series,\n",
    "                     'output_x_type': pd.core.series.Series,\n",
    "                     'output_y_type': pd.core.series.Series})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a._validate_names(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.split()\n",
    "dataset.data.keys()\n",
    "req = ['test']\n",
    "\n",
    "speller = Speller(info=['transformer', 'Speller'], request_names=req)\n",
    "lemma = Lemmatizer(info=['transformer', 'Lemmatizator'], request_names=req)\n",
    "tokenizer = Tokenizer(info=['transformer', 'Tokenizator'], request_names=req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = tokenizer.transform(dataset)\n",
    "dataset_.data['test'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastvec = FasttextVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = fastvec.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastvec.request_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastvec.new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_.data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skwrapper(BaseTransformer):\n",
    "    def __init__(self, t, config=None):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.transformer = t()\n",
    "        if not ((hasattr(t, \"fit\") or hasattr(t, \"fit_transform\")) or not hasattr(t, \"transform\")):\n",
    "            raise TypeError(\"Methods fit, fit_transform, transform are not implemented in class {} \"\n",
    "                            \"Sklearn transformers and estimators shoud implement fit and transform.\".format(t))\n",
    "\n",
    "        self.trained = False\n",
    "        params = self.transformer.get_params()\n",
    "        for key in params.keys():\n",
    "            if key in self.config.keys():\n",
    "                params[key] = self.config[key]\n",
    "\n",
    "        self.transformer.set_params(**params)\n",
    "\n",
    "\n",
    "class sktransformer(skwrapper):\n",
    "    def __init__(self, t, config=None):\n",
    "        super().__init__(t, config)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        request, report = dataset.main_names\n",
    "        if hasattr(self.transformer, 'fit_transform') and not self.trained:\n",
    "            if 'base' not in dataset.data.keys():\n",
    "                dataset.merge_data(fields_to_merge=self.request_names, delete_parent=False, new_name='base')\n",
    "                X = dataset.data['base'][request]\n",
    "                y = dataset.data['base'][report]\n",
    "                # fit\n",
    "                self.transformer.fit(X, y)\n",
    "                self.trained = True\n",
    "\n",
    "                # delete 'base' from dataset\n",
    "                dataset.del_data(['base'])\n",
    "            else:\n",
    "                X = dataset.data['base'][request]\n",
    "                y = dataset.data['base'][report]\n",
    "                # fit\n",
    "                self.transformer.fit(X, y)\n",
    "                self.trained = True\n",
    "\n",
    "            # transform all fields\n",
    "            for name, new_name in zip(self.request_names, self.new_names):\n",
    "                X = dataset.data[name][request]\n",
    "                y = dataset.data[name][report]\n",
    "                dataset.data[new_name] = {request: self.transformer.transform(X),\n",
    "                                          report: y}\n",
    "\n",
    "        else:\n",
    "            for name, new_name in zip(self.request_names, self.new_names):\n",
    "                X = dataset.data[name][request]\n",
    "                y = dataset.data[name][report]\n",
    "                dataset.data[new_name] = {request: self.transformer.transform(X),\n",
    "                                          report: y}\n",
    "\n",
    "        return dataset\n",
    "\n",
    "\n",
    "class skmodel(skwrapper):\n",
    "    def __init__(self, t, config=None):\n",
    "        super().__init__(t, config)\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        request, report = dataset.main_names\n",
    "\n",
    "        if 'train_vec' in dataset.data.keys():\n",
    "            name = 'train_vec'\n",
    "        else:\n",
    "            if 'train' in dataset.data.keys():\n",
    "                name = 'train'\n",
    "            else:\n",
    "                raise KeyError('Dataset must contain \"train_vec\" or \"train\" fields.')\n",
    "\n",
    "        X = dataset.data[name][request]\n",
    "        y = dataset.data[name][report]\n",
    "\n",
    "        if hasattr(self.transformer, 'fit') and not hasattr(self.transformer, 'fit_tranform'):\n",
    "            self.transformer.fit(X, y)\n",
    "            self.trained = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, dataset, request_names=None, new_names=None):\n",
    "\n",
    "        if not hasattr(self.transformer, 'predict'):\n",
    "            raise TypeError(\"Methods predict, is not implemented in class {} \"\n",
    "                            \" '%s' (type %s) doesn't\" % (self.transformer, type(self.transformer)))\n",
    "\n",
    "        request, report = dataset.main_names\n",
    "\n",
    "        if not self.trained:\n",
    "            raise AttributeError('Sklearn model is not trained yet.')\n",
    "\n",
    "        if (request_names is not None) and (new_names):\n",
    "            self.request_names = request_names\n",
    "            self.new_names = new_names\n",
    "\n",
    "        for name, new_name in zip(self.request_names, self.new_names):\n",
    "            X = dataset.data[name][request]\n",
    "            dataset.data[new_name] = self.transformer.predict(X)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def fit_predict(self, dataset, request_names=None, new_names=None):\n",
    "        self.fit(dataset)\n",
    "        dataset = self.predict(dataset, request_names, new_names)\n",
    "        return dataset\n",
    "\n",
    "    def predict_data(self, dataset, request_names=None, new_names=None):\n",
    "\n",
    "        if not hasattr(self.transformer, 'predict'):\n",
    "            raise TypeError(\"Methods predict, is not implemented in class {} \"\n",
    "                            \" '%s' (type %s) doesn't\" % (self.transformer, type(self.transformer)))\n",
    "\n",
    "        request, report = dataset.main_names\n",
    "\n",
    "        if not self.trained:\n",
    "            raise AttributeError('Sklearn model is not trained yet.')\n",
    "\n",
    "        if (request_names is not None) and (new_names):\n",
    "            self.request_names = request_names\n",
    "            self.new_names = new_names\n",
    "\n",
    "        res = []\n",
    "        for name, new_name in zip(self.request_names, self.new_names):\n",
    "            X = dataset.data[name][request]\n",
    "            res.append(self.transformer.predict(X))\n",
    "\n",
    "        return res\n",
    "\n",
    "    def fit_predict_data(self, dataset, request_names=None, new_names=None):\n",
    "        self.fit(dataset)\n",
    "        res = self.predict_data(dataset, request_names, new_names)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "info1 = {'op_type': 'vectorizer', 'name': 'tf-idf vectorizer',\n",
    "         'request_names': ['train', 'valid', 'test'], 'new_names': ['train_vec', 'valid_vec', 'test_vec'],\n",
    "         'input_type': pd.core.series.Series,\n",
    "         'output_type': scipy.sparse.csr.csr_matrix}\n",
    "info2 = {'op_type': 'model', 'name': 'Linear Regression',\n",
    "         'request_names': ['train_vec', 'valid_vec', 'test_vec'], 'new_names': ['valid_new', 'test_new'],\n",
    "         'input_type': scipy.sparse.csr.csr_matrix,\n",
    "         'output_type': None}\n",
    "\n",
    "clf = tfidf\n",
    "lr = LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vec = sktransformer(clf, config=info1)\n",
    "my_lr = skmodel(lr, config=info2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.split()\n",
    "\n",
    "dataset_ = my_vec.transform(dataset)\n",
    "dataset_ = my_lr.fit(dataset_).predict(dataset_,\n",
    "                                       request_names=['test_vec', 'valid_vec'],\n",
    "                                       new_names=['test_new', 'valid_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dataset_.data['test_vec']['request']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(name, config=None):\n",
    "    names = set(['speller', 'lemmatizer', 'tokenizer', 'fasttext_vectorizer',\n",
    "                 'count_vectorizer', 'tf-idf'])\n",
    "    \n",
    "    if name not in names:\n",
    "        raise TypeError('{} is not implemented.'.format(name))\n",
    "    \n",
    "    if name == 'speller':\n",
    "        return Speller(config)\n",
    "    elif name == 'lemmatizer':\n",
    "        return Lemmatizer(config)\n",
    "    elif name == 'tokenizer':\n",
    "        return Tokenizer(config)\n",
    "    elif name == 'fasttext_vectorizer':\n",
    "        return FasttextVectorizer(config)\n",
    "    elif name == 'tf-idf':\n",
    "        return sktransformer(TfidfVectorizer(config))\n",
    "    elif name == 'count_vectorizer':\n",
    "        return sktransformer(CountVectorizer(config))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "    def __init__(self, model, config):  # , fit_name=None, predict_names=None, new_names=None\n",
    "        # config resist\n",
    "        if not isinstance(config, dict):\n",
    "            raise ValueError('Input config must be dict or None, but {} was found.'.format(type(config)))\n",
    "\n",
    "        keys = ['op_type', 'name', 'fit_names', 'predict_names', 'new_names', 'input_x_type', 'input_y_type',\n",
    "                'output_x_type', 'output_y_type']\n",
    "        \n",
    "        self.info = dict()\n",
    "        \n",
    "        for x in keys:\n",
    "            if x not in config.keys():\n",
    "                raise ValueError('Input config must contain {} key.'.format(x))\n",
    "            self.info[x] = config[x]\n",
    "        \n",
    "        for x in keys:       \n",
    "            if x == 'fit_names' or x == 'predict_names' or x == 'new_names':\n",
    "                if not isinstance(config[x], list):\n",
    "                    raise ValueError('Parameters fit_names, predict_names and new_names in config must be a list,'\n",
    "                                     ' but {} \"{}\" was found.'.format(type(config[x]), config[x]))\n",
    "            \n",
    "\n",
    "        self.config = config\n",
    "        self.trained = False\n",
    "        self._validate_model(model)\n",
    "        self.model = model\n",
    "\n",
    "        # named spaces\n",
    "        self.new_names = config['new_names']\n",
    "        self.fit_names = config['fit_names']\n",
    "        self.request_names = config['predict_names']\n",
    "\n",
    "    def _validate_names(self, dataset):\n",
    "        if self.fit_names is not None:\n",
    "            for name in self.fit_names:\n",
    "                if name not in dataset.data.keys():\n",
    "                    raise KeyError('Key {} not found in dataset.'.format(name))\n",
    "        else:\n",
    "            raise KeyError('Parameter fit_names in config can not be None.')\n",
    "\n",
    "        if self.request_names is not None:\n",
    "            for name in self.request_names:\n",
    "                if name not in dataset.data.keys():\n",
    "                    raise KeyError('Key {} not found in dataset.'.format(name))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _validate_model(self, model):\n",
    "        # need_atr = ['fit', 'predict', 'fit_predict', 'save', 'restore']\n",
    "        # for atr in need_atr:\n",
    "        #     if not hasattr(model, atr):\n",
    "        #         raise AttributeError(\"Model don't supported {} method.\".format(atr))\n",
    "\n",
    "        if not (hasattr(model, 'fit') or hasattr(model, 'train')):\n",
    "            raise AttributeError(\"Model don't supported fit or train methods method.\")\n",
    "        elif not (hasattr(model, 'restore') or hasattr(model, 'load')):\n",
    "            raise AttributeError(\"Model don't supported restore or load methods method.\")\n",
    "        elif not hasattr(model, 'save'):\n",
    "            raise AttributeError(\"Model don't supported save methods method.\")\n",
    "        elif not hasattr(model, 'predict'):\n",
    "            raise AttributeError(\"Model don't supported predict or load methods method.\")\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def init_model(self, dataset):\n",
    "        self.model = self.model(self.config)\n",
    "        return self\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        self._validate_names(dataset)\n",
    "        self.init_model(dataset)\n",
    "        \n",
    "        for name in self.fit_names:\n",
    "            if hasattr(self.model, 'train'):\n",
    "                self.model.train(dataset, name)\n",
    "            if hasattr(self.model, 'fit'):\n",
    "                self.model.fit(dataset, name)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        self._validate_names(dataset)\n",
    "        self.init_model(dataset)\n",
    "        if not self.trained:\n",
    "            raise TypeError('Model is not trained yet.')\n",
    "        for name, new_name in zip(self.request_names, self.new_names):\n",
    "            dataset.data[new_name] = self.model.predict(dataset, name)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def predict_data(self, dataset):\n",
    "        self._validate_names(dataset)\n",
    "        self.init_model(dataset)\n",
    "        \n",
    "        if not self.trained:\n",
    "            raise TypeError('Model is not trained yet.')\n",
    "\n",
    "        prediction = {}\n",
    "        for name, new_name in zip(self.request_names, self.new_names):\n",
    "            prediction[new_name] = self.model.predict(dataset, name)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def fit_predict(self, dataset):\n",
    "        self.fit(dataset)\n",
    "        dataset = self.predict(dataset)\n",
    "        return dataset\n",
    "\n",
    "    def fit_predict_data(self, dataset):\n",
    "        self.fit(dataset)\n",
    "        prediction = self.predict_data(dataset)\n",
    "        return prediction\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.config\n",
    "\n",
    "    def set_params(self, params):\n",
    "        self.config = params\n",
    "        return self\n",
    "\n",
    "    def save(self, path=None):\n",
    "        if path is not None:\n",
    "            self.model.save(path)\n",
    "        else:\n",
    "            self.model.save()\n",
    "        return self\n",
    "\n",
    "    def restore(self, path=None):\n",
    "        if path is not None:\n",
    "            if isinstance(path, str):\n",
    "                self.model.restore(path)\n",
    "                self.trained = True\n",
    "            else:\n",
    "                raise TypeError('Restore path must be str, but {} was found.'.format(type(path)))\n",
    "        else:\n",
    "            self.model.restore()\n",
    "            self.trained = True\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of models wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(BaseModel):\n",
    "    def init_model(self, dataset):\n",
    "        classes = dataset.get_classes()\n",
    "        classes = ' '.join([str(x) for x in classes])\n",
    "        self.config['classes'] = classes\n",
    "        \n",
    "        self.model = self.model(self.config)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intent_classifier.intent_model.model_wrap import KerasMulticlassModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/mks/projects/intent_classification_script/configs/models/CNN/CNN_opt.json', 'r') as conf:\n",
    "    config = json.load(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'coef_reg_cnn': 0.0001,\n",
       " 'coef_reg_den': 0.0001,\n",
       " 'confident_threshold': 0.5,\n",
       " 'dense_size': 100,\n",
       " 'dropout_rate': 0.5,\n",
       " 'embedding_size': 300,\n",
       " 'epochs': 1,\n",
       " 'fasttext_model': './reddit_fasttext_model.bin',\n",
       " 'filters_cnn': 256,\n",
       " 'kernel_sizes_cnn': '1 2 3',\n",
       " 'lear_metrics': 'binary_accuracy fmeasure',\n",
       " 'lear_rate': 0.1,\n",
       " 'lear_rate_decay': 0.1,\n",
       " 'loss': 'binary_crossentropy',\n",
       " 'model_from_saved': False,\n",
       " 'model_name': 'cnn_model',\n",
       " 'model_path': './cnn_model_0',\n",
       " 'module': 'fasttext',\n",
       " 'optimizer': 'Adam',\n",
       " 'show_examples': False,\n",
       " 'text_size': 15,\n",
       " 'val_every_n_epochs': 5,\n",
       " 'val_patience': 5,\n",
       " 'verbose': True}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['op_type', 'name', 'fit_names', 'predict_names', 'new_names', 'input_x_type', 'input_y_type',\n",
    "        'output_x_type', 'output_y_type']\n",
    "\n",
    "config['op_type'] = 'model'\n",
    "config['name'] = 'cnn'\n",
    "config['fit_names'] = ['train']\n",
    "config['predict_names'] = ['test']\n",
    "config['new_names'] = ['predicted_test']\n",
    "config['input_x_type'] = str(pd.Series)\n",
    "config['input_y_type'] = str(pd.Series)\n",
    "config['output_x_type'] = str(pd.Series)\n",
    "config['output_y_type'] = str(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['fasttext_model'] = './data/russian/embeddings/ft_0.8.3_nltk_yalen_sg_300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in keys:\n",
    "    if x == 'fit_names' or x == 'predict_names' or x == 'new_names':\n",
    "        if not isinstance(config[x], list):\n",
    "            print(type(config[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(KerasMulticlassModel, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "____Training over 36221 samples____\n",
      "\n",
      "\n",
      "train -->\tupdates: 1\tloss: 0.9130334258079529\tbinary_accuracy: 0.5\tfmeasure: 0.08417507261037827\t \n",
      "train -->\tupdates: 501\tloss: 0.18026073276996613\tbinary_accuracy: 0.9586396813392639\tfmeasure: 0.47058820724487305\t \n",
      "epochs_done: 1\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'cnn_model_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d28793719248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-2cc896430ef8>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/intent_classification_script/intent_classifier/intent_model/model_wrap.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epochs_done: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/intent_classification_script/intent_classifier/intent_model/model_wrap.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0mopt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mweights_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0;31m# print(\"[ saving intent_model: {} ]\".format(str(opt_path)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mmkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparents\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(pathobj, *args)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'cnn_model_0'"
     ]
    }
   ],
   "source": [
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline(object):\n",
    "    def __init__(self, pipe):\n",
    "        self.pipe = pipe\n",
    "    \n",
    "    def fit(self, dataset):\n",
    "\n",
    "        for op in self.pipe:\n",
    "            operation = op[1]\n",
    "            if operation is not None:\n",
    "                if operation.info['op_type'] == 'transformer':\n",
    "                    dataset = operation.transform(dataset)\n",
    "                elif operation.info['op_type'] == 'vectorizer':\n",
    "                    if 'train' not in dataset.data.keys():\n",
    "                        dataset.split()\n",
    "                    operation.transform(dataset)\n",
    "                elif operation.info['op_type'] == 'model':\n",
    "                    operation.fit(dataset)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        print('[ Train End. ]')\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        prediction = None\n",
    "\n",
    "        for op in self.pipe:\n",
    "            operation = op[1]\n",
    "            if operation is not None:\n",
    "                if operation.info['op_type'] == 'transformer':\n",
    "                    dataset = operation.transform(dataset)\n",
    "                elif operation.info['op_type'] == 'vectorizer':\n",
    "                    if 'train' not in dataset.data.keys():\n",
    "                        dataset.split()\n",
    "                    operation.transform(dataset)\n",
    "                elif operation.info['op_type'] == 'model':\n",
    "                    prediction = operation.predict(dataset)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        print('[ Prediction End. ]')\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def run(self, fit_dataset, predict_dataset=None):\n",
    "        self.fit(fit_dataset)\n",
    "        if predict_dataset is None:\n",
    "            prediction = self.predict(fit_dataset)\n",
    "        else:\n",
    "            prediction = self.predict(predict_dataset)\n",
    "        \n",
    "        print('[ End. ]')\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/mks/projects/intent_classification_script/data/russian/data/vkusvill_all_categories.csv'\n",
    "global_data = read_dataset(path)\n",
    "dataset = Dataset(global_data, seed=42)\n",
    "dataset = dataset.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data['train'] = dataset.data['valid']\n",
    "dataset.del_data(['valid'])\n",
    "dataset.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = ['train', 'test']\n",
    "info1 = {'op_type': 'vectorizer', 'name': 'tf-idf vectorizer'}\n",
    "info2 = {'op_type': 'model', 'name': 'Linear Regression'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tfidf()\n",
    "lr = LogisticRegression()\n",
    "\n",
    "my_vec = sktransformer(clf, info=info1,\n",
    "                       request_names=req,\n",
    "                       new_names=['train_vec', 'test_vec'])\n",
    "my_lr = skmodel(lr, info=info2, request_names=req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speller = Speller(info=['transformer', 'Speller'], request_names=req)\n",
    "lemma = Lemmatizer(info=['transformer', 'Lemmatizator'], request_names=req)\n",
    "tokenizer = Tokenizer(info=['transformer', 'Tokenizator'], request_names=req)\n",
    "fastvec = FasttextVectorizer(info=['vectorizer', 'fasttext'], request_names=req,\n",
    "                             new_names=['train_vec', 'test_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = [('tf-idf', my_vec), ('LR', my_lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline.fit(dataset).predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_.data['test'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tiny test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_test_data = pd.DataFrame({'request': ['Это тут', \"типо такой\", \"тест офигенный\"],\n",
    "                               'report': [0, 1, 2]})\n",
    "tiny_test_dataset = Dataset(tiny_test_data)\n",
    "tiny_test_dataset.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_dataset = speller.transform(tiny_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_dataset = tokenizer.transform(tiny_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_dataset = lemma.transform(tiny_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny_dataset.split()\n",
    "tiny_dataset = fastvec.transform(tiny_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_dataset.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_dataset.data['base']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
