{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-20 21:26:32.191 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 16: Loading dictionaries from /home/mks/envs/intent_script/lib/python3.6/site-packages/pymorphy2_dicts/data\n",
      "2018-03-20 21:26:32.223 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 20: format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pymorphy2\n",
    "import fasttext\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import labels2onehot_one\n",
    "from deeppavlov.core.commands.infer import build_model_from_config\n",
    "from dataset import Dataset\n",
    "from transformer import Speller, Tokenizer, Lemmatizer, FasttextVectorizer\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# linear models\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# sklearn feachure extractors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filepath, duplicates=False, clean=True):\n",
    "    file = open(filepath, 'r', encoding='ISO-8859-1')\n",
    "    data = pd.read_csv(file)\n",
    "\n",
    "    old_names = data.keys()\n",
    "    names = [n.encode('ISO-8859-1').decode('cp1251').encode('utf8') for n in old_names]\n",
    "    names = [n.decode('utf-8') for n in names]\n",
    "\n",
    "    new_data = dict()\n",
    "    for old, new in zip(old_names, names):\n",
    "        new_data[new] = list()\n",
    "        for c in data[old]:\n",
    "            try:\n",
    "                s = c.encode('ISO-8859-1').decode('cp1251').encode('utf8')\n",
    "                s = s.decode('utf-8')\n",
    "                new_data[new].append(s)\n",
    "            except AttributeError:\n",
    "                new_data[new].append(c)\n",
    "\n",
    "    new_data = pd.DataFrame(new_data, columns=['Описание', 'Категория жалобы'])\n",
    "    new_data.rename(columns={'Описание': 'request', 'Категория жалобы': 'report'}, inplace=True)\n",
    "    new_data = new_data.dropna()  # dell nan\n",
    "    if not duplicates:\n",
    "        new_data = new_data.drop_duplicates()  # dell duplicates\n",
    "\n",
    "    # как отдельную ветвь можно использовать\n",
    "    if clean:\n",
    "        delete_bad_symbols = lambda x: \" \".join(re.sub('[^а-яa-zё0-9]', ' ', x.lower()).split())\n",
    "        new_data['request'] = new_data['request'].apply(delete_bad_symbols)\n",
    "\n",
    "    new_data = new_data.reset_index()\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from typing import Generator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, data, seed=None, classes_description=None, *args, **kwargs):\n",
    "\n",
    "        self.main_names = ['request', 'report']\n",
    "\n",
    "        rs = random.getstate()\n",
    "        random.seed(seed)\n",
    "        self.random_state = random.getstate()\n",
    "        random.setstate(rs)\n",
    "\n",
    "        self.classes_description = classes_description\n",
    "        self.data = dict()\n",
    "\n",
    "        if data.get('train') is not None:\n",
    "            self.data['train'] = data.get('train')\n",
    "        elif data.get('test') is not None:\n",
    "            self.data['test'] = data.get('test')\n",
    "        elif data.get('valid') is not None:\n",
    "            self.data['valid'] = data.get('valid')\n",
    "        else:\n",
    "            self.data['base'] = data\n",
    "\n",
    "        self.classes = self.get_classes()\n",
    "        self.classes_distribution = self.get_distribution()\n",
    "\n",
    "    def simple_split(self, splitting_proportions, field_to_split, splitted_fields, delete_parent=True):\n",
    "        data_to_div = self.data[field_to_split].copy()\n",
    "        data_size = len(self.data[field_to_split])\n",
    "        for i in range(len(splitted_fields) - 1):\n",
    "            self.data[splitted_fields[i]], data_to_div = train_test_split(data_to_div,\n",
    "                                                                          test_size=\n",
    "                                                                          len(data_to_div) -\n",
    "                                                                          int(data_size * splitting_proportions[i]))\n",
    "        self.data[splitted_fields[-1]] = data_to_div\n",
    "\n",
    "        if delete_parent:\n",
    "            a = self.data.pop(field_to_split)\n",
    "            del a\n",
    "\n",
    "        return self\n",
    "\n",
    "    def split(self, splitting_proportions=None, delete_parent=True):\n",
    "\n",
    "        dd = dict()\n",
    "        cd = self.classes_distribution\n",
    "        train = list()\n",
    "        valid = list()\n",
    "        test = list()\n",
    "\n",
    "        if splitting_proportions is None:\n",
    "            splitting_proportions = [0.1, 0.1]\n",
    "\n",
    "        if self.data.get('base', []) is not None:\n",
    "            dataset = self.data['base']\n",
    "        else:\n",
    "            raise ValueError(\"You dataset don't contains 'base' key. If You want to split a specific part dataset,\"\n",
    "                             \"please use .simple_split method.\")\n",
    "\n",
    "        for x, y in zip(dataset[self.main_names[0]], dataset[self.main_names[1]]):\n",
    "            if y not in dd.keys():\n",
    "                dd[y] = list()\n",
    "                dd[y].append((x, y))\n",
    "            else:\n",
    "                dd[y].append((x, y))\n",
    "\n",
    "        if type(splitting_proportions) is list:\n",
    "            assert len(splitting_proportions) == 2\n",
    "            assert type(splitting_proportions[0]) is float\n",
    "\n",
    "            valid_ = dict()\n",
    "            test_ = dict()\n",
    "\n",
    "            for x in dd.keys():\n",
    "                num = int(cd[x] * splitting_proportions[0])\n",
    "                valid_[x] = random.sample(dd[x], num)\n",
    "                [dd[x].remove(t) for t in valid_[x]]\n",
    "\n",
    "            for x in dd.keys():\n",
    "                num = int(cd[x] * splitting_proportions[1])\n",
    "                test_[x] = random.sample(dd[x], num)\n",
    "                [dd[x].remove(t) for t in test_[x]]\n",
    "        else:\n",
    "            raise ValueError('Split proportion must be list of floats, with length = 2')\n",
    "\n",
    "        train_ = dd\n",
    "\n",
    "        for x in train_.keys():\n",
    "            for z_, z in zip([train_, valid_, test_], [train, valid, test]):\n",
    "                z.extend(z_[x])\n",
    "\n",
    "        del train_, valid_, test_, dd, cd, dataset\n",
    "\n",
    "        for z in [train, valid, test]:\n",
    "            z = random.shuffle(z)\n",
    "\n",
    "        utrain, uvalid, utest, ctrain, cvalid, ctest = list(), list(), list(), list(), list(), list()\n",
    "        for z, n, c in zip([train, valid, test], [utrain, uvalid, utest], [ctrain, cvalid, ctest]):\n",
    "            for x in z:\n",
    "                n.append(x[0])\n",
    "                c.append(x[1])\n",
    "\n",
    "        self.data['train'] = pd.DataFrame({self.main_names[0]: utrain, self.main_names[1]: ctrain})\n",
    "        self.data['valid'] = pd.DataFrame({self.main_names[0]: uvalid, self.main_names[1]: cvalid})\n",
    "        self.data['test'] = pd.DataFrame({self.main_names[0]: utest, self.main_names[1]: ctest})\n",
    "\n",
    "        if delete_parent:\n",
    "            a = self.data.pop('base', [])\n",
    "            del a\n",
    "\n",
    "        return self\n",
    "\n",
    "    def iter_batch(self, batch_size: int, data_type: str = 'base') -> Generator:\n",
    "        \"\"\"This function returns a generator, which serves for generation of raw (no preprocessing such as tokenization)\n",
    "         batches\n",
    "        Args:\n",
    "            batch_size (int): number of samples in batch\n",
    "            data_type (str): can be either 'train', 'test', or 'valid'\n",
    "        Returns:\n",
    "            batch_gen (Generator): a generator, that iterates through the part (defined by data_type) of the dataset\n",
    "        \"\"\"\n",
    "        data = self.data[data_type]\n",
    "        data_len = len(data)\n",
    "        order = list(range(data_len))\n",
    "\n",
    "        rs = random.getstate()\n",
    "        random.setstate(self.random_state)\n",
    "        random.shuffle(order)\n",
    "        self.random_state = random.getstate()\n",
    "        random.setstate(rs)\n",
    "\n",
    "        # for i in range((data_len - 1) // batch_size + 1):\n",
    "        #     yield list(zip(*[data[o] for o in order[i * batch_size:(i + 1) * batch_size]]))\n",
    "        for i in range((data_len - 1) // batch_size + 1):\n",
    "            o = order[i * batch_size:(i + 1) * batch_size]\n",
    "            yield list((list(data[self.main_names[0]][o]), list(data[self.main_names[1]][o])))\n",
    "\n",
    "    def iter_all(self, data_type: str = 'base') -> Generator:\n",
    "        \"\"\"\n",
    "        Iterate through all data. It can be used for building dictionary or\n",
    "        Args:\n",
    "            data_type (str): can be either 'train', 'test', or 'valid'\n",
    "        Returns:\n",
    "            samples_gen: a generator, that iterates through the all samples in the selected data type of the dataset\n",
    "        \"\"\"\n",
    "        data = self.data[data_type]\n",
    "        for x, y in zip(data[self.main_names[0]], data[self.main_names[1]]):\n",
    "            yield (x, y)\n",
    "\n",
    "    def merge_data(self, fields_to_merge, delete_parent=True, new_name=None):\n",
    "        if new_name is None:\n",
    "            new_name = '_'.join([s for s in fields_to_merge])\n",
    "\n",
    "        if set(fields_to_merge) <= set(self.data.keys()):\n",
    "            fraims_to_merge = [self.data[s] for s in fields_to_merge]\n",
    "            self.data[new_name] = pd.concat(fraims_to_merge)\n",
    "        else:\n",
    "            raise KeyError('In dataset no such parts {}'.format(fields_to_merge))\n",
    "\n",
    "        if delete_parent:\n",
    "            a = [self.data.pop(x) for x in fields_to_merge]\n",
    "            del a\n",
    "\n",
    "        return self\n",
    "\n",
    "    def del_data(self, fields_to_del):\n",
    "        for name in fields_to_del:\n",
    "            a = self.data.pop(name)\n",
    "            del a\n",
    "        return self\n",
    "\n",
    "    def get_classes(self):\n",
    "        if self.data.get('base') is not None:\n",
    "            classes = self.data['base'][self.main_names[1]].unique()\n",
    "        else:\n",
    "            classes = self.data['train'][self.main_names[1]].unique()\n",
    "        return classes\n",
    "\n",
    "    def get_distribution(self):\n",
    "        try:\n",
    "            classes_distribution = self.data['base'].groupby(self.main_names[1])[self.main_names[0]].nunique()\n",
    "        except KeyError:\n",
    "            classes_distribution = self.data['train'].groupby(self.main_names[1])[self.main_names[0]].nunique()\n",
    "        return classes_distribution\n",
    "\n",
    "    def info(self):\n",
    "        information = dict(data_keys=list(self.data.keys()),\n",
    "                           classes_description=self.classes_description)\n",
    "\n",
    "        return information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mks/envs/intent_script/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2850: DtypeWarning: Columns (6,7,10,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "path = '/home/mks/projects/intent_classification_script/data/russian/data/vkusvill_all_categories.csv'\n",
    "global_data = read_dataset(path)\n",
    "dataset = Dataset(global_data, seed=42)\n",
    "dataset = dataset.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTransformer(object):\n",
    "    def __init__(self):\n",
    "        self.info = dict(type='transformer')\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "\n",
    "    def set_params(self, params):\n",
    "        # self.params = params\n",
    "        self.__init__(params)\n",
    "        return self\n",
    "\n",
    "\n",
    "class Speller(BaseTransformer):\n",
    "    def __init__(self, params=None):\n",
    "        self.info = BaseTransformer().info\n",
    "        \n",
    "        if params is None:\n",
    "            self.conf_path = '/home/mks/projects/intent_classification_script/DeepPavlov/deeppavlov/configs/error_model/brillmoore_kartaslov_ru.json'\n",
    "        else:\n",
    "            self.conf_path = params\n",
    "\n",
    "        with open(self.conf_path) as config_file:\n",
    "            self.config = json.load(config_file)\n",
    "\n",
    "        self.speller = build_model_from_config(self.config)\n",
    "\n",
    "    def transform(self, dataset, name='base'):\n",
    "        \n",
    "        print('[Speller start working ... ]')\n",
    "        \n",
    "        names = dataset.main_names\n",
    "        data = dataset.data[name]\n",
    "\n",
    "        refactor = list()\n",
    "        for x in tqdm(data[names[0]]):\n",
    "            refactor.append(self.speller([x])[0])\n",
    "\n",
    "        dataset.data[name] = pd.DataFrame({names[0]: refactor,\n",
    "                                           names[1]: data[names[1]]})\n",
    "        \n",
    "        print('[Speller done. ]')\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "\n",
    "class Tokenizer(BaseTransformer):\n",
    "    def __init__(self, params=None):\n",
    "        self.params = params\n",
    "        self.info = BaseTransformer().info\n",
    "\n",
    "    def transform(self, dataset, name='base'):\n",
    "        \n",
    "        print('[Starting tokenization ... ]')\n",
    "        \n",
    "        names = dataset.main_names\n",
    "        data = dataset.data[name][names[0]]\n",
    "\n",
    "        tok_data = list()\n",
    "        for x in tqdm(data):\n",
    "            sent_toks = nltk.sent_tokenize(x)\n",
    "            word_toks = [nltk.word_tokenize(el) for el in sent_toks]\n",
    "            tokens = [val for sublist in word_toks for val in sublist]\n",
    "            tok_data.append(tokens)\n",
    "\n",
    "        dataset.data[name] = pd.DataFrame({names[0]: tok_data,\n",
    "                                           names[1]: dataset.data[name][names[1]]})\n",
    "        \n",
    "        print('[Tokenization was done. ]')\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "\n",
    "class Lemmatizer(BaseTransformer):\n",
    "    def __init__(self, params=None):\n",
    "        self.params = params\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        self.info = BaseTransformer().info\n",
    "\n",
    "    def transform(self, dataset, name='base'):\n",
    "        print('[Starting lemmatization ... ]')\n",
    "        names = dataset.main_names\n",
    "        data = dataset.data[name][names[0]]\n",
    "\n",
    "        morph_data = list()\n",
    "        for x in tqdm(data):\n",
    "            mp_data = [self.morph.parse(el)[0].normal_form for el in x]\n",
    "            morph_data.append(mp_data)\n",
    "\n",
    "        dataset.data[name] = pd.DataFrame({names[0]: morph_data,\n",
    "                                           names[1]: dataset.data[name][names[1]]})\n",
    "        print('[Ended lemmatization. ]')\n",
    "        return dataset\n",
    "\n",
    "\n",
    "class FasttextVectorizer(BaseTransformer):\n",
    "    # TODO необходимо прописать логику того что векторизация может быть только после разделения датасета\n",
    "    def __init__(self, params=None):\n",
    "        if params is None:\n",
    "            self.params = {'path_to_model': '/home/mks/projects/intent_classification_script/data/russian/embeddings/ft_0.8.3_nltk_yalen_sg_300.bin',\n",
    "                           'dimension': 300,\n",
    "                           'file_type': 'bin'}\n",
    "\n",
    "        self.vectorizer = fasttext.load_model(self.params['path_to_model'])\n",
    "        self.info = BaseTransformer().info\n",
    "\n",
    "    def transform(self, dataset, name='base'):\n",
    "        \n",
    "        print('[Starting vectorization ... ]')\n",
    "        \n",
    "        names = dataset.main_names\n",
    "        data = dataset.data[name][names[0]]\n",
    "\n",
    "        vec_request = []\n",
    "        for x in tqdm(data):\n",
    "            matrix_i = np.zeros((len(x), self.params['dimension']))\n",
    "            for j, y in enumerate(x):\n",
    "                matrix_i[j] = self.vectorizer[y]\n",
    "            vec_request.append(matrix_i)\n",
    "\n",
    "        vec_report = list(labels2onehot_one(dataset.data[name][names[1]], dataset.classes))\n",
    "\n",
    "        dataset.data[name] = pd.DataFrame({names[0]: vec_request,\n",
    "                                           names[1]: vec_report})\n",
    "        \n",
    "        print('[Vectorization was end. ]')\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skwrapper(object):\n",
    "    def __init__(self, t, old_names=None, new_names=None):\n",
    "        \n",
    "        if (not hasattr(t, \"fit\" or not (hasattr(t, \"fit_transform\")) or hasattr(t, \"transform\"))):\n",
    "            raise TypeError(\"Methods fit, fit_transform, transform are not implemented in class {} \"\n",
    "                            \"Sklearn transformers and estimators shoud implement fit and transform.\"\n",
    "                            \" '%s' (type %s) doesn't\" % (t, type(t)))\n",
    "        \n",
    "        self.transformer = t\n",
    "        self.trained = False\n",
    "        self.info = None\n",
    "        \n",
    "        if old_names is None:\n",
    "            self.old_names = ['train', 'valid', 'test']\n",
    "        else:\n",
    "            self.old_names = old_names\n",
    "        if new_names is None:\n",
    "            self.new_names = self.old_names\n",
    "        else:\n",
    "            self.new_names = new_names\n",
    "\n",
    "\n",
    "class sktransformer_wrapper(skwrapper):  \n",
    "    def __init__(self, t, info, old_names=None, new_names=None):\n",
    "        self.info = info\n",
    "        super().__init__(t, old_names, new_names)\n",
    "    \n",
    "    def transform(self, dataset):\n",
    "        request, report = dataset.main_names\n",
    "        \n",
    "        if hasattr(self.transformer, 'fit_transform'):\n",
    "            if 'base' not in dataset.data.keys():\n",
    "                dataset.merge_data(fields_to_merge=self.old_names, delete_parent=False, new_name='base')\n",
    "            X = dataset.data['base'][request]\n",
    "            y = dataset.data['base'][report]\n",
    "            self.transformer.fit(X, y)\n",
    "            self.trained = True\n",
    "            \n",
    "            # delete 'base' from dataset\n",
    "            dataset.del_data(['base'])\n",
    "            \n",
    "            # transform all fields\n",
    "            for name, new_name in zip(self.old_names, self.new_names):\n",
    "                X = dataset.data[name][request]\n",
    "                y = dataset.data[name][report]            \n",
    "                dataset.data[new_name] = {report: self.transformer.transform(X),\n",
    "                                          report: y}\n",
    "                \n",
    "        else:     \n",
    "            for name, new_name in zip(self.old_names, self.new_names):\n",
    "                X = dataset.data[name][request]\n",
    "                y = dataset.data[name][report]            \n",
    "                dataset.data[new_name] = {report: self.transformer.transform(X),\n",
    "                                          report: y}\n",
    "            \n",
    "        return dataset\n",
    "\n",
    "    \n",
    "class skmodel_wrapper(skwrapper):\n",
    "    def __init__(self, t, info, old_names=None, new_names=None):\n",
    "        self.info = info\n",
    "        super().__init__(t, old_names, new_names)\n",
    "    \n",
    "    def fit(self, dataset):\n",
    "        self.old_names = ['train']\n",
    "        request, report = dataset.main_names\n",
    "        \n",
    "        for name in self.old_names:\n",
    "            X = dataset.data[name][request]\n",
    "            y = dataset.data[name][report]\n",
    "\n",
    "            if hasattr(self.transformer, 'fit') and not hasattr(self.transformer, 'fit_tranform'):\n",
    "                self.transformer.fit(X, y)\n",
    "                self.trained = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, dataset):\n",
    "        \n",
    "        if not hasattr(self.transformer, 'predict'):\n",
    "            raise TypeError(\"Methods predict, is not implemented in class {} \"\n",
    "                            \" '%s' (type %s) doesn't\" % (self.transformer, type(self.transformer)))\n",
    "        \n",
    "        request, report = dataset.main_names\n",
    "        \n",
    "        if not self.trained:\n",
    "            # TODO write correct error\n",
    "            raise ValueError('Sklearn model is not trained yet.')\n",
    "        \n",
    "        for name, new_name in zip(self.old_names, self.new_names):\n",
    "            X = dataset.data[name][request]\n",
    "#             y = dataset.data[name][report]\n",
    "\n",
    "            dataset.data[new_name] = self.transformer.predict(X)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def fit_predict(self, dataset):      \n",
    "        self.fit(dataset)\n",
    "        dataset = self.predict(dataset)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type \"<class 'dict'>\"; only pd.Series, pd.DataFrame, and pd.Panel (deprecated) objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-f2a4bb201991>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmy_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskmodel_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfo2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdataset_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# dataset_ = my_lr.fit(dataset_, old_name='vec').predict(dataset_, old_name='vec', new_name='new')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-154423c99319>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'base'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields_to_merge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mold_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelete_parent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'base'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'base'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-020015e0259f>\u001b[0m in \u001b[0;36mmerge_data\u001b[0;34m(self, fields_to_merge, delete_parent, new_name)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields_to_merge\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mfraims_to_merge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields_to_merge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfraims_to_merge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'In dataset no such parts {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields_to_merge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/intent_script/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    210\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                        copy=copy)\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/intent_script/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    270\u001b[0m                        \u001b[0;34m' only pd.Series, pd.DataFrame, and pd.Panel'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                        ' (deprecated) objs are valid'.format(type(obj)))\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type \"<class 'dict'>\"; only pd.Series, pd.DataFrame, and pd.Panel (deprecated) objs are valid"
     ]
    }
   ],
   "source": [
    "info1 = {'op_type': 'vectorizer', 'name': 'tf-idf vectorizer'}\n",
    "info2 = {'op_type': 'model', 'name': 'Linear Regression'}\n",
    "\n",
    "clf = tfidf()\n",
    "my_vec = sktransformer_wrapper(clf, info=info1)\n",
    "lr = LogisticRegression()\n",
    "my_lr = skmodel_wrapper(lr, info=info2)\n",
    "\n",
    "dataset_ = my_vec.transform(dataset)\n",
    "# dataset_ = my_lr.fit(dataset_, old_name='vec').predict(dataset_, old_name='vec', new_name='new')\n",
    "\n",
    "dataset_.data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid', 'test'])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(name, config=None):\n",
    "    names = set(['speller', 'lemmatizer', 'tokenizer', 'fasttext_vectorizer',\n",
    "                 'count_vectorizer', 'tf-idf'])\n",
    "    \n",
    "    if name not in names:\n",
    "        raise TypeError('{} is not implemented.'.format(name))\n",
    "    \n",
    "    if name == 'speller':\n",
    "        return Speller(config)\n",
    "    elif name == 'lemmatizer':\n",
    "        return Lemmatizer(config)\n",
    "    elif name == 'tokenizer':\n",
    "        return Tokenizer(config)\n",
    "    elif name == 'fasttext_vectorizer':\n",
    "        return FasttextVectorizer(config)\n",
    "    elif name == 'tf-idf':\n",
    "        return skwrapper(TfidfVectorizer(config))\n",
    "    elif name == 'count_vectorizer':\n",
    "        return skwrapper(CountVectorizer(config))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-20 14:48:19.730 INFO in 'deeppavlov.vocabs.typos'['typos'] at line 76: Loading a dictionary from /home/mks/projects/intent_classification_script/DeepPavlov/download/russian_words_vocab\n",
      "2018-03-20 14:48:25.510 INFO in 'deeppavlov.models.spellers.error_model.error_model'['error_model'] at line 239: loading error_model from `/home/mks/projects/intent_classification_script/DeepPavlov/download/error_model/error_model_ru.tsv`\n"
     ]
    }
   ],
   "source": [
    "a = initialization('speller')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline(object):\n",
    "    def __init__(self, pipe):\n",
    "        self.pipe = pipe\n",
    "    \n",
    "    def fit(self, dataset, name, **fit_params):\n",
    "\n",
    "        for op in self.pipe:\n",
    "            operation = op[1]\n",
    "            if operation is not None:\n",
    "                if operation.info['type'] == 'transformer':\n",
    "                    dataset = operation.transform(dataset, name=name)\n",
    "                elif operation.info['type'] == 'model':\n",
    "                    operation.init(dataset)\n",
    "                    operation.fit()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        return self, dataset\n",
    "\n",
    "    def predict(self, dataset, name, **fit_params):\n",
    "        prediction = None\n",
    "\n",
    "        for op in self.pipe:\n",
    "            operation = op[1]\n",
    "            if operation is not None:\n",
    "                if operation.info['type'] == 'transformer':\n",
    "                    dataset = operation.transform(dataset, name=name)\n",
    "                elif operation.info['type'] == 'model':\n",
    "                    operation.init(dataset)\n",
    "                    prediction = operation.predict(dataset)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = [('Speller', Speller()), ('Tokenizer', Tokenizer())]\n",
    "pipe2 = [('Lemmatizer', Lemmatizer()), ('Vectorizer', FasttextVectorizer())]\n",
    "pipe3 = [('Tokenizer', Tokenizer()), ('Lemmatizer', Lemmatizer()), ('Vectorizer', FasttextVectorizer())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(pipe3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 442/4475 [00:00<00:00, 4416.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Starting tokenization ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:00<00:00, 5160.24it/s]\n",
      "  0%|          | 11/4475 [00:00<00:40, 109.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokenization was done. ]\n",
      "[Starting lemmatization ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:26<00:00, 166.19it/s]\n",
      "  4%|▎         | 162/4475 [00:00<00:02, 1606.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ended lemmatization. ]\n",
      "[Starting vectorization ... ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:02<00:00, 1734.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vectorization was end. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline, dataset_ = pipeline.fit(dataset, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report</th>\n",
       "      <th>request</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[-0.18812957406044006, -0.0639662817120552, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[0.12598323822021484, 0.1466168761253357, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[-0.5365741848945618, 0.01435030810534954, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[-0.2726789116859436, 0.21540087461471558, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[0.06901668012142181, 0.14644865691661835, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              report  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                             request  \n",
       "0  [[-0.18812957406044006, -0.0639662817120552, -...  \n",
       "1  [[0.12598323822021484, 0.1466168761253357, -0....  \n",
       "2  [[-0.5365741848945618, 0.01435030810534954, 0....  \n",
       "3  [[-0.2726789116859436, 0.21540087461471558, -0...  \n",
       "4  [[0.06901668012142181, 0.14644865691661835, -0...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_.data['test'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
